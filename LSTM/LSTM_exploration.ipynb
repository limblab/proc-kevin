{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM decoding exploration\n",
    "\n",
    "Going to be looking at using LSTMs (with potentially some changes to the cost function) to decode EMGs from cortical data. This is all based off work from Steph and Josh.\n",
    "\n",
    "\n",
    "Going to start with the old Jango data, then update from there as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 13:43:32.744881: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-26 13:43:32.852985: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-26 13:43:32.853001: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-26 13:43:32.872501: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-26 13:43:33.428642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-26 13:43:33.428716: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-26 13:43:33.428724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import least_squares\n",
    "from matplotlib import pyplot as plt\n",
    "# import ipympl\n",
    "\n",
    "# we'll use ridge regression as a comparisson\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "from tkinter import Tk\n",
    "from tkinter import filedialog as fd # just so I don't have to repeatedly manually enter filenames\n",
    "\n",
    "# and tf stuff\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K # this was in Josh's version. Not sure why can't use numpy\n",
    "\n",
    "# datetime stuff for logs\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x7feafb5edfd0>\n"
     ]
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "# import ipympl\n",
    "%matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request the filename\n",
    "root = Tk()\n",
    "mat_fn = fd.askopenfilename(master=root,filetypes=[('matlab data','*.mat')])\n",
    "root.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different loading code if it's data from Josh vs XDS stuff\n",
    "\n",
    "**Josh**\n",
    "It looks like this will input a dictionary containing (among other things) numpy arrays for each of the datasets for the day. We'll pull in each of them, and parse accordingly.\n",
    "\n",
    "In the \"binned\" numpy arrays (two levels down), the organization seems to be:\n",
    "\n",
    "0. timestamps\n",
    "1. metadata\n",
    "2. EMG names\n",
    "3. EMG data\n",
    "4. Force names\n",
    "5. Force data\n",
    "6. electrode and unit number\n",
    "7. channel and unit number\n",
    "8. Firing Rates (in hz)\n",
    "9. Kin (cursor kin?) names\n",
    "10. Kin (cursor kin?) data\n",
    "11. Vel Data\n",
    "12. Vel Names\n",
    "13. Accel Data\n",
    "14. Accel Names\n",
    "15. Digital Words and timestamps\n",
    "16. Targets:\n",
    "\n",
    "    0. corner locations and appearance time \n",
    "    1. rotations and appearance time\n",
    "\n",
    "17. Trial table data\n",
    "18. Trial Table Labels\n",
    "19. \n",
    "20. \n",
    "21. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load it in\n",
    "data = loadmat(mat_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_josh_mat(curr_data):\n",
    "    timestamps = curr_data[0][0][0].reshape(-1)\n",
    "    # create a couple of pandas data frames -- this will make things a bit easier\n",
    "\n",
    "    # EMG\n",
    "    emg_names = [curr_data[0][0][2][ii][0][0] for ii in np.arange(len(curr_data[0][0][2]))]\n",
    "    EMG = pd.DataFrame(curr_data[0][0][3], columns=emg_names, index=timestamps)\n",
    "\n",
    "    # Forces\n",
    "    force_names = [curr_data[0][0][4][ii][0][0] for ii in np.arange(len(curr_data[0][0][4]))]\n",
    "    force = pd.DataFrame(curr_data[0][0][5], columns=force_names, index=timestamps)\n",
    "\n",
    "    # Firing Rates\n",
    "    channel_names = [f\"cc{row[0]:03d}ee{row[1]}\" for row in curr_data[0][0][7]]\n",
    "    firing = pd.DataFrame(curr_data[0][0][8], columns=channel_names, index=timestamps)\n",
    "\n",
    "    # Kin -- not sure if cursor or ang of wrist. Will need to check\n",
    "    kin_names = curr_data[0][0][9]\n",
    "    vel_names = curr_data[0][0][12]\n",
    "    acc_names = curr_data[0][0][14]\n",
    "    kin = pd.DataFrame(curr_data[0][0][10], columns=kin_names, index=timestamps)\n",
    "    try:\n",
    "        kin = kin.join(pd.DataFrame(curr_data[0][0][11], columns=vel_names, index=timestamps))\n",
    "        kin = kin.join(pd.DataFrame(curr_data[0][0][13], columns=acc_names, index=timestamps))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # trial table\n",
    "    trial_names = curr_data[0][0][18]\n",
    "    trial_table = pd.DataFrame(curr_data[0][0][17], columns=trial_names)\n",
    "\n",
    "    return timestamps, firing, EMG, force, kin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DoneText\n",
    "A quick script that will send me a text message when I ask it to -- so that I can walk away and work on other things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import smtplib\n",
    "import ssl\n",
    "from os import getenv\n",
    "\n",
    "# To send a text when you've finished everything\n",
    "class doneText():\n",
    "    def __init__(self):\n",
    "        self.notif_email = getenv('notif_email') # source for this email\n",
    "        self.notif_pass = getenv('notif_pass')   # pass for the source email\n",
    "        self.notif_target = getenv('notif_target') # target email [phone_num]@[mmsgateway]\n",
    "\n",
    "    def send(self, message):\n",
    "        ssl_context = ssl.create_default_context()\n",
    "        with smtplib.SMTP_SSL('smtp.gmail.com', 465, context=ssl_context) as server:\n",
    "            server.login(self.notif_email, self.notif_pass)\n",
    "            result = server.sendmail(self.notif_email, self.notif_target, f\"Subject:\\n{message}\")\n",
    "\n",
    "            print(f\"Text send result: {result}\")\n",
    "            server.quit()\n",
    "\n",
    "\n",
    "dntxt = doneText() # initialize an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "def doneTone():\n",
    "    low_A = 440\n",
    "    # C_sharp = int(440* 2**(4/12))\n",
    "    # E = int(440* 2**(7/12))\n",
    "    # hi_A = int(440* 2)\n",
    "\n",
    "    winsound.Beep(low_A, 500)\n",
    "    # winsound.Beep(E, 500)\n",
    "    # winsound.Beep(C_sharp, 500)\n",
    "    # winsound.Beep(hi_A, 1500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the data\n",
    "\n",
    "Plot out some good information on the max and variances of each muscle. Should also look at something for the cortex, maybe depth of modulation or avg firing rates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_names = ['WmTrain','SprTrain','IsoTrain']\n",
    "test_names = ['WmTest','SprTest','IsoTest']\n",
    "\n",
    "_, WmTrain_firing, WmTrain_EMG, _, _ = load_josh_mat(data['WmTrain'])\n",
    "_, WmTest_firing, WmTest_EMG, _, _ = load_josh_mat(data['WmTest'])\n",
    "_, IsoTrain_firing, IsoTrain_EMG, _, _ = load_josh_mat(data['IsoTrain'])\n",
    "_, IsoTest_firing, IsoTest_EMG, _, _ = load_josh_mat(data['IsoTest'])\n",
    "_, SprTrain_firing, SprTrain_EMG, _, _ = load_josh_mat(data['SprTrain'])\n",
    "_, SprTest_firing, SprTest_EMG, _, _ = load_josh_mat(data['SprTest'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bar plots of representative values (max and 95th pctl) of different EMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# Max EMG values bar plot\n",
    "\n",
    "fig_emg_max, ax_emg_max = plt.subplots()\n",
    "i_muscles = np.arange(WmTrain_EMG.shape[1]) # indexing on the x axis\n",
    "bar_width = .25\n",
    "\n",
    "\n",
    "ax_emg_max.bar(i_muscles, np.max(WmTrain_EMG,axis=0), width = bar_width, label='Movement')\n",
    "ax_emg_max.bar(i_muscles+bar_width, np.max(IsoTrain_EMG, axis=0), width = bar_width, label='Isometric')\n",
    "ax_emg_max.bar(i_muscles+bar_width*2, np.max(SprTrain_EMG, axis=0), width = bar_width, label='Spring')\n",
    "\n",
    "ax_emg_max.set_xticks(i_muscles+bar_width)\n",
    "ax_emg_max.set_xticklabels(WmTrain_EMG.columns)\n",
    "\n",
    "fig_emg_max.show()\n",
    "_ = ax_emg_max.legend()\n",
    "\n",
    "ax_emg_max.set_xlabel('Muscle')\n",
    "ax_emg_max.set_ylabel('Max Value')\n",
    "\n",
    "# For each bar in the chart, add a text label.\n",
    "for bar in ax_emg_max.patches:\n",
    "    # The text annotation for each bar should be its height.\n",
    "    bar_value = bar.get_height()\n",
    "    # Format the text with commas to separate thousands. You can do\n",
    "    # any type of formatting here though.\n",
    "    text = f'{bar_value:.02f}'\n",
    "    # This will give the middle of each bar on the x-axis.\n",
    "    text_x = bar.get_x() + bar.get_width() / 2\n",
    "    # get_y() is where the bar starts so we add the height to it.\n",
    "    text_y = np.max([bar.get_y() + bar_value, 0.01]) # keep it from going too far below zero, and disappearing\n",
    "    # If we want the text to be the same color as the bar, we can\n",
    "    # get the color like so:\n",
    "    bar_color = bar.get_facecolor()\n",
    "    # If you want a consistent color, you can just set it as a constant, e.g. #222222\n",
    "    ax_emg_max.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,\n",
    "            size=12)\n",
    "    \n",
    "for spine in ['right','top','bottom','left']:\n",
    "    ax_emg_max.spines[spine].set_visible(False)\n",
    "\n",
    "ax_emg_max.set_title('Maximum EMG Value')\n",
    "\n",
    "# --------------------------------------------\n",
    "# 95th percentile\n",
    "fig_emg_95, ax_emg_95 = plt.subplots()\n",
    "i_muscles = np.arange(WmTrain_EMG.shape[1]) # indexing on the x axis\n",
    "bar_width = .25\n",
    "\n",
    "\n",
    "ax_emg_95.bar(i_muscles, np.percentile(WmTrain_EMG, 95,axis=0), width = bar_width, label='Movement')\n",
    "ax_emg_95.bar(i_muscles+bar_width, np.percentile(IsoTrain_EMG, 95, axis=0), width = bar_width, label='Isometric')\n",
    "ax_emg_95.bar(i_muscles+2*bar_width, np.percentile(SprTrain_EMG, 95, axis=0), width = bar_width, label='Spring')\n",
    "\n",
    "ax_emg_95.set_xticks(i_muscles+bar_width)\n",
    "ax_emg_95.set_xticklabels(WmTrain_EMG.columns)\n",
    "\n",
    "# fig_emg_95.show()\n",
    "_ = ax_emg_95.legend()\n",
    "\n",
    "ax_emg_95.set_xlabel('Muscle')\n",
    "ax_emg_95.set_ylabel('95th Percentile')\n",
    "\n",
    "# For each bar in the chart, add a text label.\n",
    "for bar in ax_emg_95.patches:\n",
    "    # The text annotation for each bar should be its height.\n",
    "    bar_value = bar.get_height()\n",
    "    # Format the text with commas to separate thousands. You can do\n",
    "    # any type of formatting here though.\n",
    "    text = f'{bar_value:.02f}'\n",
    "    # This will give the middle of each bar on the x-axis.\n",
    "    text_x = bar.get_x() + bar.get_width() / 2\n",
    "    # get_y() is where the bar starts so we add the height to it.\n",
    "    text_y = np.max([bar.get_y() + bar_value, 0.01]) # keep it from going too far below zero, and disappearing\n",
    "    # If we want the text to be the same color as the bar, we can\n",
    "    # get the color like so:\n",
    "    bar_color = bar.get_facecolor()\n",
    "    # If you want a consistent color, you can just set it as a constant, e.g. #222222\n",
    "    ax_emg_95.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,\n",
    "            size=12)\n",
    "\n",
    "\n",
    "for spine in ['right','top','bottom','left']:\n",
    "    ax_emg_95.spines[spine].set_visible(False)\n",
    "\n",
    "ax_emg_95.set_title('95th Percentile of EMG')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing mean firing rates of different conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_cort, ax_cort = plt.subplots(ncols=3)\n",
    "\n",
    "Wm_means = np.mean(WmTrain_firing, axis=0)\n",
    "Iso_means = np.mean(IsoTrain_firing, axis=0)\n",
    "Spr_means = np.mean(SprTrain_firing, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# scatters between mean firing rates\n",
    "ax_cort[0].scatter(Wm_means, Iso_means, s=2)\n",
    "ax_cort[0].plot([0,40],[0,40],c='k', alpha=.3)\n",
    "ax_cort[1].scatter(Wm_means, Spr_means, s=2)\n",
    "ax_cort[1].plot([0,40],[0,40],c='k', alpha=.3)\n",
    "ax_cort[2].scatter(Iso_means, Spr_means, s=2)\n",
    "ax_cort[2].plot([0,40],[0,40],c='k', alpha=.3)\n",
    "\n",
    "\n",
    "# x and y labels\n",
    "ax_cort[0].set_xlabel('Movement')\n",
    "ax_cort[0].set_ylabel('Isometric')\n",
    "ax_cort[1].set_xlabel('Movement')\n",
    "ax_cort[1].set_ylabel('Spring')\n",
    "ax_cort[2].set_xlabel('Isometric')\n",
    "ax_cort[2].set_ylabel('Spring')\n",
    "\n",
    "# making the axes square and removing the spines\n",
    "for axis in ax_cort:\n",
    "    axis.set_aspect('equal',adjustable='box')\n",
    "    for spine in ['right','top','bottom','left']:\n",
    "        ax_emg_95.spines[spine].set_visible(False)\n",
    "    axis.set_title('Mean Firing Rates')\n",
    "\n",
    "\n",
    "fig_cort.set_label('Compared Mean Firing Rates across conditions')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same with the variance of the firing rates (thinking depth of modulation sort of thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WmTrain_firing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m fig_cort, ax_cort \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m Wm_vars \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvar(\u001b[43mWmTrain_firing\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      4\u001b[0m Iso_vars \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvar(IsoTrain_firing, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m Spr_vars \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mvar(SprTrain_firing, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WmTrain_firing' is not defined"
     ]
    }
   ],
   "source": [
    "fig_cort, ax_cort = plt.subplots(ncols=3)\n",
    "\n",
    "Wm_vars = np.var(WmTrain_firing, axis=0)\n",
    "Iso_vars = np.var(IsoTrain_firing, axis=0)\n",
    "Spr_vars = np.var(SprTrain_firing, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "# scatters between var firing rates\n",
    "ax_cort[0].scatter(Wm_vars, Iso_vars, s=2)\n",
    "ax_cort[1].scatter(Wm_vars, Spr_vars, s=2)\n",
    "ax_cort[2].scatter(Iso_vars, Spr_vars, s=2)\n",
    "\n",
    "\n",
    "# x and y labels\n",
    "ax_cort[0].set_xlabel('Movement')\n",
    "ax_cort[0].set_ylabel('Isometric')\n",
    "ax_cort[1].set_xlabel('Movement')\n",
    "ax_cort[1].set_ylabel('Spring')\n",
    "ax_cort[2].set_xlabel('Isometric')\n",
    "ax_cort[2].set_ylabel('Spring')\n",
    "\n",
    "# making the axes square and removing the spines\n",
    "for axis in ax_cort:\n",
    "    axis.set_aspect('equal',adjustable='box')\n",
    "    max_lim = np.max([axis.get_xlim()[1],axis.get_ylim()[1]])\n",
    "    min_lim = np.min([axis.get_xlim()[0],axis.get_ylim()[1]])\n",
    "    axis.set_xlim([min_lim, max_lim])\n",
    "    axis.set_ylim([min_lim, max_lim])\n",
    "    for spine in ['right','top','bottom','left']:\n",
    "        axis.spines[spine].set_visible(False)\n",
    "    axis.set_title('Variance of Firing Rates')\n",
    "    axis.plot([min_lim, max_lim],[min_lim, max_lim],c='k', alpha=.3)\n",
    "\n",
    "fig_cort.set_label('Compared var Firing Rates across conditions')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting functions\n",
    "\n",
    "We'll want to plot the test vs predicted data, and the VAFs. Since this should be consistent across training types, we'll just define the plots as a function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_values(EMG_test,EMG_pred):\n",
    "\n",
    "    n_rows = int(np.ceil(np.sqrt(EMG_test.shape[1])))\n",
    "    fig_lstm, ax_lstm = plt.subplots(nrows=n_rows, ncols=n_rows, sharex=True, constrained_layout=True)\n",
    "\n",
    "    for muscle_ii,muscle_n in train_EMG.itercols:\n",
    "        row_i = int(muscle_ii//n_rows)\n",
    "        col_i = int(muscle_ii%n_rows)\n",
    "        ax_lstm[row_i,col_i].plot(test_timestamps, test_EMG.iloc[:,muscle_ii]*EMG_std[ii], label='Recorded')\n",
    "        ax_lstm[row_i,col_i].plot(test_timestamps,test_pred[:,muscle_ii]*EMG_std[ii], label=f'LSTM VAF: {test_VAFs[muscle_ii]:.03f}')\n",
    "        ax_lstm[row_i,col_i].set_title(f\"{test_EMG.columns[muscle_ii]}\")\n",
    "        _ = ax_lstm[row_i,col_i].legend()\n",
    "\n",
    "        # turn off the spines\n",
    "        for spine in ['right','top','bottom','left']:\n",
    "            ax_lstm[row_i,col_i].spines[spine].set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_VAFs(train_VAFs: dict, test_VAFs: dict, muscles):    \n",
    "    \n",
    "    # Plotting VAFs -- comparing the training sets and testing sets\n",
    "    fig_vaf, ax_vaf = plt.subplots(ncols = 2, constrained_layout=True)\n",
    "    i_muscles = np.arange(len(muscles)) # indexing on the x axis\n",
    "    n_train = len(train_VAFs)\n",
    "    n_test = len(test_VAFs)\n",
    "    train_width = (8//n_train)*.1 # the bars together should only take 80% of each muscle's width\n",
    "    test_width = (8//n_test)*.1\n",
    "    \n",
    "    # training plot\n",
    "    for train_ii, vaf in enumerate(train_VAFs.items()):\n",
    "        ax_vaf[0].bar(i_muscles + train_width*train_ii, vaf[1], width=train_width, label=vaf[0])\n",
    "    ax_vaf[0].set_xticks(i_muscles + .4)\n",
    "    ax_vaf[0].set_xticklabels(muscles)\n",
    "    ax_vaf[0].set_title('Training datasets')\n",
    "    \n",
    "    # test plot\n",
    "    for test_ii, vaf in enumerate(test_VAFs.items()):                              \n",
    "        ax_vaf[1].bar(i_muscles + test_width*test_ii, vaf[1], width = test_width, label=vaf[0])\n",
    "    ax_vaf[1].set_xticks(i_muscles + .4)\n",
    "    ax_vaf[1].set_xticklabels(muscles)\n",
    "    ax_vaf[1].set_title('Testing Datasets')\n",
    "\n",
    "\n",
    "\n",
    "    # For each bar in the chart, add a text label.\n",
    "    for chart in ax_vaf:\n",
    "        chart.set_ylim([-.05, 1.05])\n",
    "        chart.set_xlabel('Muscle')\n",
    "        chart.set_ylabel('R2 Score')\n",
    "\n",
    "        chart.legend()\n",
    "\n",
    "        for bar in chart.patches:\n",
    "            # The text annotation for each bar should be its height.\n",
    "            bar_value = bar.get_height()\n",
    "            # Format the text with commas to separate thousands. You can do\n",
    "            # any type of formatting here though.\n",
    "            text = f'{bar_value:.02f}'\n",
    "            # This will give the middle of each bar on the x-axis.\n",
    "            text_x = bar.get_x() + bar.get_width() / 2\n",
    "            # get_y() is where the bar starts so we add the height to it.\n",
    "            text_y = np.max([bar.get_y() + bar_value, 0.01]) # keep it from going too far below zero, and disappearing\n",
    "            # If we want the text to be the same color as the bar, we can\n",
    "            # get the color like so:\n",
    "            bar_color = bar.get_facecolor()\n",
    "            # If you want a consistent color, you can just set it as a constant, e.g. #222222\n",
    "            chart.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,\n",
    "                    size=12)\n",
    "\n",
    "        # turn off the spines\n",
    "        for spine in ['right','top','bottom','left']:\n",
    "            chart.spines[spine].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building \n",
    "Look through a couple of different model types, look to see how they are trained\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear with Static Non-linearity\n",
    "\n",
    "Using the lab default -- build a wiener filter, fit it, then fit with a static polynomial on top of the form\n",
    "\n",
    "$Ax^2 + Bx + C$\n",
    "\n",
    "Where $x$ is the original EMG prediction, and the output is the new EMG prediction\n",
    "\n",
    "Alternatively, I have also allowed to predict using an exponential activation \n",
    "\n",
    "$Ae^{Bx} + C$\n",
    "\n",
    "Also giving the options for a sigmoid\n",
    "\n",
    "\n",
    "**Starting with defining our nonlinearity methods**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using scipy's least_squares:\n",
    "def non_linearity(p, y_pred, nonlinear_type):\n",
    "    if nonlinear_type == 'poly':\n",
    "        return p[0] + p[1]*y_pred + p[2]*y_pred**2\n",
    "    elif nonlinear_type == 'exponential':\n",
    "        return p[0]*np.exp(p[1]*y_pred) + p[2]\n",
    "    elif nonlinear_type == 'sigmoid':\n",
    "        return p[1] * 1/(1 + np.exp(-10*(y_pred-p[0])))\n",
    "\n",
    "def non_linearity_residuals(p, y_pred, y_act, nonlinear_type):\n",
    "    if nonlinear_type == 'poly':\n",
    "        return y_act - (p[0] + p[1]*y_pred + p[2]*y_pred**2)\n",
    "    elif nonlinear_type == 'exponential':\n",
    "        return y_act - (p[0]*np.exp(p[1]*y_pred) + p[2])\n",
    "    elif nonlinear_type == 'sigmoid':\n",
    "        return y_act - (p[1] * 1/(1 + np.exp(-10*(y_pred-p[0]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next a function that compares Wiener filter models with Wiener cascades, reports the VAF (Cooefficient of Determination) and gives plots for the validation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a function that we can just call multiple times, so then we can just quickly run through all of the different combinations\n",
    "\n",
    "\n",
    "def basic_decoder_comparison(train_firing, train_EMG, retrain_firing, retrain_EMG, test_firing, test_EMG, test_timestamps, nonlinear_type, save_plot=True):\n",
    "    wiener_input = pd.DataFrame() # empty dataframe\n",
    "    n_lags = 10 # number of lags\n",
    "    for ii in np.arange(n_lags): # create the lagged dataframe\n",
    "        col_dict = dict(zip(train_firing.columns, train_firing.columns+f\"_lag{ii}\"))\n",
    "        wiener_input = wiener_input.join(train_firing.shift(-ii, fill_value=0).rename(columns=col_dict), how='outer')\n",
    "\n",
    "    wiener_test = pd.DataFrame() # empty dataframe\n",
    "    for ii in np.arange(n_lags): # create the lagged dataframe\n",
    "        col_dict = dict(zip(test_firing.columns, test_firing.columns+f\"_lag{ii}\"))\n",
    "        wiener_test = wiener_test.join(test_firing.shift(-ii, fill_value=0).rename(columns=col_dict), how='outer')\n",
    "        \n",
    "    wiener_retrain = pd.DataFrame() # empty dataframe\n",
    "    for ii in np.arange(n_lags): # create the lagged dataframe\n",
    "        col_dict = dict(zip(retrain_firing.columns, retrain_firing.columns+f\"_lag{ii}\"))\n",
    "        wiener_retrain = wiener_retrain.join(retrain_firing.shift(-ii, fill_value=0).rename(columns=col_dict), how='outer')\n",
    "\n",
    "\n",
    "\n",
    "    if nonlinear_type == 'poly':\n",
    "        init_pred = [.1, .1, .1]\n",
    "    elif nonlinear_type == 'exponential':\n",
    "        init_pred = [1, .1, .2]\n",
    "    elif nonlinear_type == 'sigmoid':\n",
    "        init_pred = [.1, .5]\n",
    "\n",
    "    mdl_A = linear_model.LinearRegression(fit_intercept=True)\n",
    "    mdl_B = {} # a dictionary of numpy polynomials. Probably a better way to do this, maybe Xuan's method... oh well\n",
    "\n",
    "    mdl_A.fit(wiener_input, train_EMG)# fit the first model\n",
    "    prefit_EMG = mdl_A.predict(wiener_input) # get the initially predicted EMGs\n",
    "    retrain_pred = mdl_A.predict(wiener_retrain) # from a training set on a separate condition -- to properly hold out test data\n",
    "    prefit_VAF = metrics.explained_variance_score(train_EMG, prefit_EMG, multioutput='raw_values')\n",
    "    nonlin_EMG = np.zeros(prefit_EMG.shape)\n",
    "\n",
    "    print('Training Results\\n')\n",
    "\n",
    "    for ii in np.arange(len(train_EMG.columns)):\n",
    "        muscle = train_EMG.columns[ii]\n",
    "        mdl_B[muscle] = least_squares(non_linearity_residuals, init_pred, args=(prefit_EMG[:,ii], train_EMG.iloc[:,ii].to_numpy(), nonlinear_type)).x\n",
    "        # print('--------------------------------------------------------')\n",
    "        print(muscle)\n",
    "        print(f\"\\tLinear VAF: {prefit_VAF[ii]:.03f}\")\n",
    "        nonlin_EMG[:,ii] = non_linearity(mdl_B[muscle],(prefit_EMG[:,ii]), nonlinear_type)\n",
    "        print(f\"\\tNonLinear VAF: {metrics.explained_variance_score(train_EMG.iloc[:,ii],nonlin_EMG[:,ii]):.03f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # predicting the test set\n",
    "    prefit_test = mdl_A.predict(wiener_test)\n",
    "    prefit_test_VAF = metrics.explained_variance_score(test_EMG, prefit_test, multioutput='raw_values')\n",
    "    nonlin_test = np.zeros(prefit_test.shape)\n",
    "    nonlin_within_test = np.zeros(prefit_test.shape)\n",
    "    nonlin_VAF = np.zeros(prefit_test_VAF.shape)\n",
    "    nonlin_within_VAF = np.zeros(prefit_test_VAF.shape)\n",
    "    mdl_C = {} # for a separate non-linearity, built for the second condition.\n",
    "\n",
    "    print('\\n------------------------------------------------------------\\n')\n",
    "    print('Testing Results\\n')\n",
    "    for ii in np.arange(len(train_EMG.columns)):\n",
    "        muscle = test_EMG.columns[ii]\n",
    "        mdl_C[muscle] = least_squares(non_linearity_residuals, init_pred, args=(retrain_pred[:,ii], retrain_EMG.iloc[:,ii].to_numpy(), nonlinear_type)).x\n",
    "        # print('--------------------------------------------------------')\n",
    "        print(muscle)\n",
    "        print(f\"\\tLinear VAF: {prefit_test_VAF[ii]:.03f}\")\n",
    "        nonlin_test[:,ii] = non_linearity(mdl_B[muscle],(prefit_test[:,ii]), nonlinear_type)\n",
    "        nonlin_within_test[:,ii] = non_linearity(mdl_C[muscle],(prefit_test[:,ii]), nonlinear_type)\n",
    "        nonlin_VAF[ii] = metrics.explained_variance_score(test_EMG.iloc[:,ii],nonlin_test[:,ii])\n",
    "        nonlin_within_VAF[ii] = metrics.explained_variance_score(test_EMG.iloc[:,ii],nonlin_within_test[:,ii])\n",
    "        print(f\"\\tPre-built Nonlinearity VAF: {nonlin_VAF[ii]:.03f}\")\n",
    "        print(f\"\\tRe-built Nonlinearity VAF: {nonlin_within_VAF[ii]:.03f}\")\n",
    "\n",
    "    # Plotting the test data -- so that we can see how  the non-linearities act between types\n",
    "    n_rows = int(np.ceil(np.sqrt(len(train_EMG.columns))))\n",
    "    fig_nl_test, ax_nl_test = plt.subplots(nrows=n_rows, ncols=n_rows, sharex=True, constrained_layout=True)\n",
    "\n",
    "    for muscle_ii in np.arange(len(train_EMG.columns)):\n",
    "        row_i = int(muscle_ii//n_rows)\n",
    "        col_i = int(muscle_ii%n_rows)\n",
    "        ax_nl_test[row_i,col_i].plot(test_timestamps, test_EMG.iloc[:,muscle_ii], label='Recorded')\n",
    "        ax_nl_test[row_i,col_i].plot(test_timestamps,prefit_test[:,muscle_ii], label=f'Linear VAF: {prefit_test_VAF[muscle_ii]:.03f}')\n",
    "        ax_nl_test[row_i,col_i].plot(test_timestamps,nonlin_test[:,muscle_ii], label=f'NonLin VAF: {nonlin_VAF[muscle_ii]:.03f}')\n",
    "        ax_nl_test[row_i,col_i].plot(test_timestamps,nonlin_within_test[:,muscle_ii], label=f'Rebuilt NonLin VAF: {nonlin_within_VAF[muscle_ii]:.03f}')\n",
    "        ax_nl_test[row_i,col_i].set_title(f\"{test_EMG.columns[muscle_ii]}\")\n",
    "        ax_nl_test[row_i,col_i].set_xlabel(f\"Time (s)\")\n",
    "        ax_nl_test[row_i,col_i].set_ylabel(\"EMG envelope\")\n",
    "        \n",
    "        _ = ax_nl_test[row_i,col_i].legend()\n",
    "\n",
    "        # turn off the spines\n",
    "        for spine in ['right','top','bottom','left']:\n",
    "            ax_nl_test[row_i,col_i].spines[spine].set_visible(False)\n",
    "            \n",
    "    if save_plot:\n",
    "        fig_nl_test.savefig('Wiener_Cascade_Comparison.svg')\n",
    "\n",
    "    # let's also plot the VAFs in a clean manner so that it's easy to compare\n",
    "    fig_vaf, ax_vaf = plt.subplots()\n",
    "    i_muscles = np.arange(len(test_EMG.columns)) # indexing on the x axis\n",
    "    bar_width = .25\n",
    "\n",
    "    ax_vaf.bar(i_muscles, prefit_test_VAF, width = bar_width, label='Linear')\n",
    "    ax_vaf.bar(i_muscles + bar_width, nonlin_VAF, width = bar_width, label='Nonlinear')\n",
    "    ax_vaf.bar(i_muscles + 2*bar_width, nonlin_within_VAF, width = bar_width, label='Rebuilt Nonlinear')\n",
    "\n",
    "    ax_vaf.set_xticks(i_muscles + 1*bar_width)\n",
    "    ax_vaf.set_xticklabels(test_EMG.columns)\n",
    "\n",
    "    ax_vaf.set_ylim([-.05, 1.05])\n",
    "    ax_vaf.set_xlabel('Muscle')\n",
    "    ax_vaf.set_ylabel('Coefficient of Determination')\n",
    "\n",
    "    ax_vaf.legend()\n",
    "\n",
    "    # For each bar in the chart, add a text label.\n",
    "    for bar in ax_vaf.patches:\n",
    "        # The text annotation for each bar should be its height.\n",
    "        bar_value = bar.get_height()\n",
    "        # Format the text with commas to separate thousands. You can do\n",
    "        # any type of formatting here though.\n",
    "        text = f'{bar_value:.02f}'\n",
    "        # This will give the middle of each bar on the x-axis.\n",
    "        text_x = bar.get_x() + bar.get_width() / 2\n",
    "        # get_y() is where the bar starts so we add the height to it.\n",
    "        text_y = np.max([bar.get_y() + bar_value, 0.01]) # keep it from going too far below zero, and disappearing\n",
    "        # If we want the text to be the same color as the bar, we can\n",
    "        # get the color like so:\n",
    "        bar_color = bar.get_facecolor()\n",
    "        # If you want a consistent color, you can just set it as a constant, e.g. #222222\n",
    "        ax_vaf.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,\n",
    "                size=12)\n",
    "\n",
    "    # turn off the spines\n",
    "    for spine in ['right','top','bottom','left']:\n",
    "        ax_vaf.spines[spine].set_visible(False)\n",
    "\n",
    "\n",
    "    if save_plot:\n",
    "        fig_vaf.savefig('Wiener_VAF.svg')\n",
    "\n",
    "    return nonlin_EMG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_comparisons(train_firing, train_EMG, test_firing, test_EMG, test_timestamps, plot=True, save_plot=True):\n",
    "    # hyper params\n",
    "    layer_0_units = 300\n",
    "    drop_in = .25     # input dropout percentage for LSTM layer\n",
    "    drop_rec = 0    # recurrent dropout for LSTM\n",
    "    drop_lay = .15    # dropout layer?\n",
    "\n",
    "    # # input hyper params -- reshape input vector\n",
    "    # batch_size = 64 # why not? will test to see training accuracy after\n",
    "    # seq_len = train_firing.shape[0]//batch_size # splitting out the batches\n",
    "    n_neurons = train_firing.shape[1] # number of neurons\n",
    "    n_EMGs = train_EMG.shape[1]\n",
    "    seq_len = 12\n",
    "    rnn_train_i = np.zeros((train_firing.shape[0],seq_len, train_firing.shape[1]))\n",
    "    rnn_test_i = np.zeros((test_firing.shape[0],seq_len, test_firing.shape[1]))\n",
    "    rnn_train_o = train_EMG.to_numpy()\n",
    "    rnn_test_o = test_EMG.to_numpy()\n",
    "    \n",
    "    # normalize the EMGs\n",
    "    EMG_std = np.std(rnn_train_o, axis=0)\n",
    "    for ii in np.arange(rnn_train_o.shape[1]):\n",
    "        rnn_train_o[:,ii] = rnn_train_o[:,ii]/EMG_std[ii]\n",
    "        rnn_test_o[:,ii] = rnn_test_o[:,ii]/EMG_std[ii]\n",
    "    \n",
    "    # create the sequences\n",
    "    for ii in np.arange(10):\n",
    "        rnn_train_i[:,ii,:] = train_firing.shift(-ii, fill_value=0).to_numpy()\n",
    "        rnn_test_i[:,ii,:] = test_firing.shift(-ii, fill_value=0).to_numpy()\n",
    "\n",
    "    # Set up the LSTMs\n",
    "    mdl = tf.keras.models.Sequential()\n",
    "\n",
    "\n",
    "    mdl.add(tf.keras.layers.LSTM(layer_0_units, input_shape = (seq_len,n_neurons), dropout=drop_in, recurrent_dropout=drop_rec))\n",
    "    if drop_lay:\n",
    "        mdl.add(tf.keras.layers.Dropout(drop_lay)) # dropout layer if wanted\n",
    "#     mdl.add(tf.keras.layers.Dense(train_EMG.shape[1], activation='relu')) # dense combination layer\n",
    "    mdl.add(tf.keras.layers.Dense(train_EMG.shape[1])) # dense combination layer\n",
    "    mdl.compile(loss='mse', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    \n",
    "    mdl.fit(rnn_train_i, rnn_train_o, epochs=20, verbose=False)\n",
    "    \n",
    "    \n",
    "    train_pred = mdl.predict(rnn_train_i)\n",
    "    test_pred = mdl.predict(rnn_test_i)\n",
    "    train_VAFs = metrics.explained_variance_score(rnn_train_o, train_pred, multioutput='raw_values')\n",
    "    test_VAFs = metrics.explained_variance_score(rnn_test_o, test_pred, multioutput='raw_values')\n",
    "    \n",
    "    print('----------------------------------------')\n",
    "    for ii in np.arange(len(train_EMG.columns)):\n",
    "        print(train_EMG.columns[ii])\n",
    "        print(f\"\\tTrain VAF: {train_VAFs[ii]}\")\n",
    "        print(f\"\\tTest VAF: {test_VAFs[ii]}\")\n",
    "    \n",
    "    \n",
    "    # plottin\n",
    "    \n",
    "    if plot:\n",
    "        n_rows = int(np.ceil(np.sqrt(len(train_EMG.columns))))\n",
    "        fig_lstm, ax_lstm = plt.subplots(nrows=n_rows, ncols=n_rows, sharex=True, constrained_layout=True)\n",
    "\n",
    "        for muscle_ii in np.arange(len(train_EMG.columns)):\n",
    "            row_i = int(muscle_ii//n_rows)\n",
    "            col_i = int(muscle_ii%n_rows)\n",
    "            ax_lstm[row_i,col_i].plot(test_timestamps, test_EMG.iloc[:,muscle_ii]*EMG_std[ii], label='Recorded')\n",
    "            ax_lstm[row_i,col_i].plot(test_timestamps,test_pred[:,muscle_ii]*EMG_std[ii], label=f'LSTM VAF: {test_VAFs[muscle_ii]:.03f}')\n",
    "            ax_lstm[row_i,col_i].set_title(f\"{test_EMG.columns[muscle_ii]}\")\n",
    "            _ = ax_lstm[row_i,col_i].legend()\n",
    "\n",
    "            # turn off the spines\n",
    "            for spine in ['right','top','bottom','left']:\n",
    "                ax_lstm[row_i,col_i].spines[spine].set_visible(False)\n",
    "\n",
    "        if save_plot:\n",
    "            fig_lstm.savefig('LSTM_traces.svg')\n",
    "                \n",
    "                \n",
    "        # let's also plot the VAFs in a clean manner so that it's easy to compare\n",
    "        fig_vaf, ax_vaf = plt.subplots()\n",
    "        i_muscles = np.arange(len(test_EMG.columns)) # indexing on the x axis\n",
    "        bar_width = .3\n",
    "\n",
    "        ax_vaf.bar(i_muscles, train_VAFs, width = bar_width, label='Train')\n",
    "        ax_vaf.bar(i_muscles + bar_width, test_VAFs, width = bar_width, label='Test')\n",
    "\n",
    "        ax_vaf.set_xticks(i_muscles + bar_width/2)\n",
    "        ax_vaf.set_xticklabels(test_EMG.columns)\n",
    "\n",
    "        ax_vaf.set_ylim([-.05, 1.05])\n",
    "        ax_vaf.set_xlabel('Muscle')\n",
    "        ax_vaf.set_ylabel('VAF')\n",
    "\n",
    "        ax_vaf.legend()\n",
    "\n",
    "        # For each bar in the chart, add a text label.\n",
    "        for bar in ax_vaf.patches:\n",
    "            # The text annotation for each bar should be its height.\n",
    "            bar_value = bar.get_height()\n",
    "            # Format the text with commas to separate thousands. You can do\n",
    "            # any type of formatting here though.\n",
    "            text = f'{bar_value:.02f}'\n",
    "            # This will give the middle of each bar on the x-axis.\n",
    "            text_x = bar.get_x() + bar.get_width() / 2\n",
    "            # get_y() is where the bar starts so we add the height to it.\n",
    "            text_y = np.max([bar.get_y() + bar_value, 0.01]) # keep it from going too far below zero, and disappearing\n",
    "            # If we want the text to be the same color as the bar, we can\n",
    "            # get the color like so:\n",
    "            bar_color = bar.get_facecolor()\n",
    "            # If you want a consistent color, you can just set it as a constant, e.g. #222222\n",
    "            ax_vaf.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,\n",
    "                    size=12)\n",
    "\n",
    "        # turn off the spines\n",
    "        for spine in ['right','top','bottom','left']:\n",
    "            ax_vaf.spines[spine].set_visible(False)\n",
    "\n",
    "\n",
    "        if save_plot:\n",
    "            fig_vaf.savefig('LSTM_VAF.svg')\n",
    "\n",
    "\n",
    "    return test_VAFs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Custom Loss functions\n",
    "\n",
    "First the weighted loss function. This one calculates the MSE, but weights the value for each point in time by the variance of that particular task. This balances the training so that the model is able to train for conditions with different EMG ranges (the whole idea behind our system...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight the losses by the std of that particular range in time and muscle.\n",
    "# for hybrid decoders\n",
    "def hybrid_weight_loss(target, pred):\n",
    "    # inputs: \n",
    "    #         target is the recorded data, plus the weights since this needs to be callable by tf\n",
    "    #                First half of the columns of the data will be EMG, second half will be the weights\n",
    "    #         pred   is the current prediction values\n",
    "    num_targets = K.shape(target)[1]//2 # number of cols / 2\n",
    "    err = (target[:, 0:num_targets] - pred[:,0:num_targets]) # subtract the values\n",
    "    se = K.square(err) * target[:,num_targets:] # multiply the square error by the gains\n",
    "    mse = K.mean(se, axis=-1)\n",
    "#     tf.print(f\"Error Shape: {mse.shape}\")\n",
    "    \n",
    "    return mse\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the variance Risk Extrapolation (V-REx) from Krueger et al 2021. The purpose of this is to maximize the out-of-distribution generalization. This places a penalty on the variance of the risk (MSE as defined...)\n",
    "\n",
    "$ R_{V-REx}(\\theta) = \\beta * \\sigma^2(R_1(\\theta)....R_m(\\theta)) + \\sum_{e=1}^{m}R_e(\\theta) $\n",
    "\n",
    "Where risk is defined as \n",
    " $ R^2 = \\sum_{i=0}^{I}\\frac{(\\hat{y_i}-y_i)^2}{I} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vREx loss function\n",
    "def vrex_loss(target, pred):\n",
    "    # from the Risk Extrapolation paper\n",
    "    B = 0 #\n",
    "    # B = .5 #\n",
    "    # B = 1 #\n",
    "#     B = 5 #\n",
    "    # B = 10 #\n",
    "    # B = 50 #\n",
    "    # B = 100 #\n",
    "    \n",
    "    err = target[:,:-1] - pred[:,:-1] # without the condition flag\n",
    "    se_musc = K.square(err) # squared error\n",
    "    mse_musc = tf.expand_dims(K.mean(se_musc, axis=-1),-1) # mean squared error for each sample\n",
    "    \n",
    "    # now create a one-hot matrix for timestamps with a one for the appropriate condition\n",
    "    conditions = target[:,-1] # condition flag\n",
    "    u_conds,i_conds = tf.unique(conditions)\n",
    "    cond_oh = tf.transpose(tf.one_hot(i_conds, len(u_conds))) # create a mask the same size as the risk\n",
    "    \n",
    "    # split up the risks (MSE for right now) per condition\n",
    "    sum_risks = tf.matmul(cond_oh,mse_musc) # this gives us the per-condition sum of MSEs\n",
    "    risks = tf.math.divide(sum_risks, K.sum(cond_oh, axis=1)) # and this should average them\n",
    "\n",
    "    \n",
    "    rex = B*K.var(risks) + K.sum(risks) # and now to run the risk extrapolation step\n",
    "\n",
    "    return rex\n",
    "    # return mse_musc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vrex_weighted(target, pred):\n",
    "    # weighting the risks based on the var of the EMG\n",
    "    # For this, we will bring in an array of Tx(2*M+1) \n",
    "    # where M is the number of muscles and the extra column is for a flag\n",
    "\n",
    "    # define the balance of mean and variance of risk\n",
    "    B = 0 #\n",
    "#     B = .5 #\n",
    "#     B = 1 #\n",
    "#     B = 5 #\n",
    "#     B = 10 #\n",
    "#     B = 50 #\n",
    "#     B = 100 #\n",
    "    \n",
    "    # number of muscles\n",
    "    muscles = target.shape[1]//2 # find that value of M\n",
    "    \n",
    "    \n",
    "    err = target[:,:muscles] - pred[:,:muscles] # find the error\n",
    "    se_musc = K.square(err) # square error per muscle per timepoint\n",
    "    se = tf.expand_dims(K.sum(tf.multiply(err, target[:,muscles:-1]),axis=1),axis=-1) # take a weighted sum  of muscles per ts\n",
    "#     tf.print(f\"se shape:{se.shape}\")\n",
    "    \n",
    "    \n",
    "    # separate the conditions based on the flag\n",
    "    flag = target[:,-1] # last column\n",
    "#     risks = [K.mean(se[flag==C]) for C,_ in tf.unique(flag)]\n",
    "    u_flag,i_flag = tf.unique(flag) # for another one-hot matrix\n",
    "    flag_mask = tf.one_hot(i_flag, len(u_flag), axis=0) # mask for each condition -- CxT\n",
    "#     tf.print(f\"flag_mask shape: {flag_mask.shape}\")\n",
    "\n",
    "    split_se = tf.matmul(flag_mask, se) # this should give us a Cx1 array\n",
    "    # need to get the mean per condition, and samples/condition changes\n",
    "    # depending on the batch. the flag mask has already counted for us :)\n",
    "    risks = tf.math.divide(split_se, K.sum(flag_mask, axis=0))\n",
    "#     split_risk = tf.matmul(var_oh, mse_musc)\n",
    "#     weighted_risk = tf.multiply(tf.expand_dims(u_var, axis=-1), split_risk)\n",
    "#     risks = tf.math.divide(weighted_risk, K.sum(var_oh, axis=1)) # get the mean\n",
    "\n",
    "    rex = B*K.var(risks) + K.sum(risks)\n",
    "\n",
    "#     return rex\n",
    "    return K.mean(se_musc, axis=-1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_across_weighted(train_firing, train_EMG, test_firing, test_EMG, test_timestamps, plot=True):\n",
    "    if type(train_firing) is not list:\n",
    "        print(f\"training data must be a list, not a {type(train_firing)}\")\n",
    "        return -1\n",
    "    \n",
    "    # hyper params\n",
    "    layer_0_units = 300\n",
    "    drop_in = .25     # input dropout percentage for LSTM layer\n",
    "    drop_rec = 0    # recurrent dropout for LSTM\n",
    "    drop_lay = .15    # dropout layer?\n",
    "    \n",
    "    # size and name of outputs\n",
    "    n_target = train_EMG[0].shape[1] # dimensionality of target (EMGs usually)\n",
    "    col_names = train_EMG[0].columns\n",
    "    \n",
    "    \n",
    "    # put together the training targets \n",
    "    # This means both the EMGs and the variance for that portion of the training set\n",
    "    loss_weights = np.ndarray((0,n_target)) # weights for the training losses\n",
    "    rnn_train_o = np.ndarray((0,n_target*2)) # rnn target for training set. Going to also contain the weights\n",
    "    rnn_test_o = test_EMG.to_numpy() # don't need the weights for the test set\n",
    "    \n",
    "    # initialize the input sequence data\n",
    "    n_neurons = train_firing[0].shape[1] # number of neurons\n",
    "    seq_len = 10\n",
    "    rnn_train_i = np.ndarray((0,seq_len, n_neurons))\n",
    "    rnn_test_i = np.ndarray((test_firing.shape[0], seq_len, n_neurons))\n",
    "    \n",
    "    # training target set\n",
    "    for target in train_EMG:\n",
    "        # put together the weights -- the variances, since we're doing MSE as our training loss\n",
    "        temp_var = np.matmul(np.ones((target.shape[0],1)),np.var(target.to_numpy(), axis=0).reshape(1,-1))\n",
    "#         loss_weights = np.append(loss_weights,temp_var, axis=0)\n",
    "        # append the training targets\n",
    "        temp_train = np.append(target.to_numpy(), 1./temp_var, axis=1)\n",
    "        rnn_train_o = np.append(rnn_train_o, temp_train, axis=0)\n",
    "    \n",
    "    \n",
    "    # testing target set -- have to append the neural firing\n",
    "    for train in train_firing:\n",
    "        temp_train_i = np.ndarray((train.shape[0], seq_len, n_neurons))\n",
    "        for ii in np.arange(seq_len):\n",
    "            temp_train_i[:,ii,:] = train.shift(-ii, fill_value=0).to_numpy()\n",
    "        rnn_train_i = np.append(rnn_train_i, temp_train_i, axis=0)\n",
    "            \n",
    "    for ii in np.arange(seq_len):\n",
    "        rnn_test_i[:,ii,:] = test_firing.shift(-ii, fill_value=0).to_numpy()\n",
    "        \n",
    "    \n",
    "        # Set up the LSTMs\n",
    "    mdl = tf.keras.models.Sequential()\n",
    "\n",
    "\n",
    "    mdl.add(tf.keras.layers.LSTM(layer_0_units, input_shape = (seq_len,n_neurons), dropout=drop_in, recurrent_dropout=drop_rec))\n",
    "    if drop_lay:\n",
    "        mdl.add(tf.keras.layers.Dropout(drop_lay)) # dropout layer if wanted\n",
    "#     mdl.add(tf.keras.layers.Dense(n_target*2, activation='relu')) # dense combination layer -- x2 to account for weights\n",
    "    mdl.add(tf.keras.layers.Dense(n_target*2)) # try with linear -- how does it compare?\n",
    "    mdl.compile(loss=hybrid_weight_loss, optimizer='rmsprop', metrics=['MeanAbsoluteError'])\n",
    "    \n",
    "#     mdl.fit(rnn_train_i, rnn_train_o, epochs=20, batch_size=256, verbose=True)\n",
    "    mdl.fit(rnn_train_i, rnn_train_o, epochs=40, verbose=False)\n",
    "    \n",
    "    \n",
    "    train_pred = mdl.predict(rnn_train_i)[:,:n_target]\n",
    "    test_pred = mdl.predict(rnn_test_i)[:,:n_target]\n",
    "    train_VAFs = metrics.explained_variance_score(rnn_train_o[:,:n_target], train_pred, multioutput='raw_values')\n",
    "    test_VAFs = metrics.explained_variance_score(rnn_test_o, test_pred, multioutput='raw_values')\n",
    "    \n",
    "    print('----------------------------------------')\n",
    "    for ii in np.arange(n_target):\n",
    "        print(col_names[ii])\n",
    "        print(f\"\\tTrain VAF: {train_VAFs[ii]}\")\n",
    "        print(f\"\\tTest VAF: {test_VAFs[ii]}\")\n",
    "    \n",
    "    if plot:\n",
    "        # plottin\n",
    "        n_rows = int(np.ceil(np.sqrt(n_target)))\n",
    "        fig_lstm, ax_lstm = plt.subplots(nrows=n_rows, ncols=n_rows, sharex=True, constrained_layout=True)\n",
    "\n",
    "        for muscle_ii in np.arange(n_target):\n",
    "            row_i = int(muscle_ii//n_rows)\n",
    "            col_i = int(muscle_ii%n_rows)\n",
    "            ax_lstm[row_i,col_i].plot(test_timestamps, rnn_test_o[:,muscle_ii], label='Recorded')\n",
    "            ax_lstm[row_i,col_i].plot(test_timestamps,test_pred[:,muscle_ii], label=f'LSTM VAF: {test_VAFs[muscle_ii]:.03f}')\n",
    "            ax_lstm[row_i,col_i].set_title(f\"{col_names[muscle_ii]}\")\n",
    "            _ = ax_lstm[row_i,col_i].legend()\n",
    "\n",
    "            # turn off the spines\n",
    "            for spine in ['right','top','bottom','left']:\n",
    "                ax_lstm[row_i,col_i].spines[spine].set_visible(False)\n",
    "    \n",
    "    \n",
    "        # let's also plot the VAFs in a clean manner so that it's easy to compare\n",
    "        fig_vaf, ax_vaf = plt.subplots()\n",
    "        i_muscles = np.arange(n_target) # indexing on the x axis\n",
    "        bar_width = .3\n",
    "\n",
    "        ax_vaf.bar(i_muscles, train_VAFs, width = bar_width, label='Train')\n",
    "        ax_vaf.bar(i_muscles + bar_width, test_VAFs, width = bar_width, label='Test')\n",
    "\n",
    "        ax_vaf.set_xticks(i_muscles + bar_width/2)\n",
    "        ax_vaf.set_xticklabels(col_names)\n",
    "\n",
    "        ax_vaf.set_ylim([-.05, 1.05])\n",
    "        ax_vaf.set_xlabel('Muscle')\n",
    "        ax_vaf.set_ylabel('VAF')\n",
    "\n",
    "        ax_vaf.legend()\n",
    "\n",
    "        # For each bar in the chart, add a text label.\n",
    "        for bar in ax_vaf.patches:\n",
    "            # The text annotation for each bar should be its height.\n",
    "            bar_value = bar.get_height()\n",
    "            # Format the text with commas to separate thousands. You can do\n",
    "            # any type of formatting here though.\n",
    "            text = f'{bar_value:.02f}'\n",
    "            # This will give the middle of each bar on the x-axis.\n",
    "            text_x = bar.get_x() + bar.get_width() / 2\n",
    "            # get_y() is where the bar starts so we add the height to it.\n",
    "            text_y = np.max([bar.get_y() + bar_value, 0.01]) # keep it from going too far below zero, and disappearing\n",
    "            # If we want the text to be the same color as the bar, we can\n",
    "            # get the color like so:\n",
    "            bar_color = bar.get_facecolor()\n",
    "            # If you want a consistent color, you can just set it as a constant, e.g. #222222\n",
    "            ax_vaf.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,\n",
    "                    size=12)\n",
    "\n",
    "        # turn off the spines\n",
    "        for spine in ['right','top','bottom','left']:\n",
    "            ax_vaf.spines[spine].set_visible(False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tensorboard so that we can easily explore\n",
    "# %load_ext tensorboard\n",
    "\n",
    "\n",
    "def LSTM_grid_search(firing, EMG, n_iter = 150, n_fold = 10, n_epochs = 20, unit_range = [100, 400], drop_in_range = [0,.5], drop_rec_range = [0,.5], drop_lay_range = [0,.5], seq_range=[5,20], plot=True):\n",
    "    # Runs a monte-carlo style grid search on hyper parameters\n",
    "    # It will run mfxval, so there is no need for a separate training group\n",
    "    #\n",
    "    #  This will allow me to compare the number of lstm units, drop percentages,\n",
    "    #  batch and sequence sizes, etc.\n",
    "    \n",
    "    EMG_names = EMG.columns\n",
    "    cols = [f\"{name}_train_VAF\" for name in EMG_names]\n",
    "    cols += [f'{name}_test_VAF' for name in EMG_names]\n",
    "    cols += ['n_units','drop_in','drop_rec','drop_lay','seq_len']\n",
    "    log = pd.DataFrame(columns=cols)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs/LSTM_exploration', histogram_freq=1)\n",
    "\n",
    "    \n",
    "    n_neurons = firing.shape[1] # number of neurons\n",
    "    n_EMGs = EMG.shape[1]\n",
    "    \n",
    "    # get the indices of the folds\n",
    "    kf = KFold(n_splits=n_fold, random_state=None, shuffle=False) # working in chunks, not random indices\n",
    "#     train_idx,test_idx = kf.split(firing) # get the indices. Won't split until later, since we will need to set up the sequences each round\n",
    "    \n",
    "    # intiialize the random number generator for the monte carlo\n",
    "    rng = np.random.default_rng()\n",
    "    \n",
    "    for iter in np.arange(n_iter):\n",
    "        layer_0_units = rng.integers(unit_range[0],unit_range[1])\n",
    "        drop_in = rng.uniform(drop_in_range[0], drop_in_range[1])\n",
    "        drop_rec = rng.uniform(drop_rec_range[0], drop_rec_range[1])\n",
    "        drop_lay = rng.uniform(drop_lay_range[0], drop_lay_range[1])\n",
    "        seq_len = rng.integers(seq_range[0], seq_range[1])\n",
    "        \n",
    "        rnn_i = np.ndarray((firing.shape[0], seq_len, firing.shape[1]))\n",
    "        for ii in np.arange(seq_len):\n",
    "            rnn_i[:,ii,:] = firing.shift(-ii, fill_value=0).to_numpy()\n",
    "        \n",
    "        \n",
    "        train_VAFs = np.zeros((n_fold, n_EMGs))\n",
    "        test_VAFs = np.zeros((n_fold, n_EMGs))\n",
    "        \n",
    "        log_entry = {'n_units':layer_0_units, 'drop_in':drop_in, 'drop_rec':drop_rec,\\\n",
    "                        'drop_lay':drop_lay, 'seq_len':seq_len}\n",
    "        \n",
    "        fold_idx = 0\n",
    "        for train_idx,test_idx in kf.split(firing):\n",
    "\n",
    "            rnn_train_i = np.zeros((len(train_idx),seq_len, firing.shape[1]))\n",
    "            rnn_test_i = np.zeros((len(test_idx),seq_len, firing.shape[1]))\n",
    "            rnn_train_o = EMG.iloc[train_idx,:].to_numpy()\n",
    "            rnn_test_o = EMG.iloc[test_idx,:].to_numpy()\n",
    "            \n",
    "            # split training and testing inputs\n",
    "            rnn_train_i = rnn_i[train_idx,:,:]\n",
    "            rnn_test_i = rnn_i[test_idx,:,:]\n",
    "\n",
    "\n",
    "            # normalize the EMGs\n",
    "            EMG_std = np.std(rnn_train_o, axis=0)\n",
    "            for ii in np.arange(rnn_train_o.shape[1]):\n",
    "                rnn_train_o[:,ii] = rnn_train_o[:,ii]/EMG_std[ii]\n",
    "                rnn_test_o[:,ii] = rnn_test_o[:,ii]/EMG_std[ii]\n",
    "\n",
    "            # Set up the LSTMs\n",
    "            mdl = tf.keras.models.Sequential()\n",
    "\n",
    "\n",
    "            mdl.add(tf.keras.layers.LSTM(layer_0_units, input_shape = (seq_len,n_neurons), dropout=drop_in, recurrent_dropout=drop_rec))\n",
    "            if drop_lay:\n",
    "                mdl.add(tf.keras.layers.Dropout(drop_lay)) # dropout layer if wanted\n",
    "        #     mdl.add(tf.keras.layers.Dense(n_EMGs, activation='relu')) # dense combination layer\n",
    "            mdl.add(tf.keras.layers.Dense(n_EMGs)) # dense combination layer\n",
    "            mdl.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "            mdl.fit(rnn_train_i, rnn_train_o, epochs=n_epochs, verbose=False, callbacks=[tensorboard_callback])\n",
    "\n",
    "\n",
    "            train_pred = mdl.predict(rnn_train_i, verbose=False)\n",
    "            test_pred = mdl.predict(rnn_test_i, verbose=False)\n",
    "            train_VAFs[fold_idx,:] = metrics.explained_variance_score(rnn_train_o, train_pred, multioutput='raw_values')\n",
    "            test_VAFs[fold_idx,:] = metrics.explained_variance_score(rnn_test_o, test_pred, multioutput='raw_values')\n",
    "            \n",
    "            fold_idx += 1\n",
    "            \n",
    "        # store the VAFs in the log entry\n",
    "        for emg_iter,emg_name in enumerate(EMG_names):\n",
    "            log_entry[f\"{emg_name}_train_VAF\"] = np.mean(train_VAFs[:,emg_iter])\n",
    "            log_entry[f\"{emg_name}_test_VAF\"] = np.mean(test_VAFs[:,emg_iter])\n",
    "        \n",
    "        \n",
    "#         return log_entry\n",
    "#         print(pd.DataFrame.from_records([log_entry]))\n",
    "        log = pd.concat([log,pd.DataFrame.from_records([log_entry])], ignore_index=True)\n",
    "        print(f\"Looped iteration {iter} of {n_iter}\")\n",
    "        \n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_rex(train_firing: dict, train_EMG: dict, test_firing: dict, test_EMG: dict, test_timestamps, plot=True):\n",
    "    if type(train_firing) is not dict:\n",
    "        print(f\"train_firing must be a dict of Numpy Arrays, not a {type(train_firing)}\")\n",
    "        return -1\n",
    "        \n",
    "    \n",
    "    # hyper params\n",
    "    layer_0_units = 300\n",
    "    drop_in = .25     # input dropout percentage for LSTM layer\n",
    "    drop_rec = 0    # recurrent dropout for LSTM\n",
    "    drop_lay = .15    # dropout layer?\n",
    "    \n",
    "    # size and name of outputs\n",
    "    train_sets = list(train_EMG.keys())\n",
    "    n_target = train_EMG[train_sets[0]].shape[1] # dimensionality of target (EMGs usually)\n",
    "    col_names = train_EMG[train_sets[0]].columns\n",
    "    \n",
    "\n",
    "    # tensorboard callback\n",
    "    log_dir = f'C:\\\\Users\\\\17204\\\\Documents\\\\Tensorboard_logs\\\\LSTM_logs\\\\{dt.today().strftime(\"%Y%m%d\")}_{\"REx_unweighted\"}'\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    \n",
    "\n",
    "    # start by initializing the input sequence data\n",
    "    n_neurons = train_firing[train_sets[0]].shape[1]\n",
    "    seq_len = 10\n",
    "    rnn_train_i = np.ndarray((0,seq_len, n_neurons))\n",
    "    \n",
    "    # testing target set -- have to append the neural firing\n",
    "    train_i = {} # keep the separated and the joined training stuff\n",
    "    for train_name,train_data in train_firing.items():\n",
    "        temp_train_i = np.ndarray((train_data.shape[0], seq_len, n_neurons))\n",
    "        for ii in np.arange(seq_len):\n",
    "            temp_train_i[:,ii,:] = train_data.shift(-ii, fill_value=0).to_numpy()\n",
    "        train_i[train_name] = temp_train_i # store it for later -- won't have to resort\n",
    "        rnn_train_i = np.append(rnn_train_i, temp_train_i, axis=0)\n",
    "    train_i['combined'] = rnn_train_i\n",
    "    \n",
    "    \n",
    "    rnn_train_o = np.ndarray((0,n_target+1)) # initialize the training outputs\n",
    "    train_o = {}\n",
    "    for ii_EMG, EMG in enumerate(train_EMG.items()): # iterate through list\n",
    "        temp_train = EMG[1].to_numpy() # add the numpy stuff\n",
    "        temp_train = np.append(temp_train, ii_EMG+1 * np.ones((EMG[1].shape[0],1)), axis=-1) # append a category flag to the last column\n",
    "        train_o[EMG[0]] = temp_train # store for later\n",
    "        rnn_train_o = np.append(rnn_train_o, temp_train, axis=0)\n",
    "    train_o['combined'] = rnn_train_o\n",
    "\n",
    "    # normalize EMGs\n",
    "#     EMG_std = np.std(rnn_train_o, axis=0)\n",
    "#     for ii in np.arange(rnn_train_o.shape[1] - 1):\n",
    "#         rnn_train_o[:,ii] = rnn_train_o[:,ii]/EMG_std[ii]\n",
    "#         rnn_test_o[:,ii] = rnn_test_o[:,ii]/EMG_std[ii]\n",
    "\n",
    "\n",
    "    # create the model\n",
    "    mdl = tf.keras.models.Sequential()\n",
    "\n",
    "    # add the LSTM layer\n",
    "    mdl.add(tf.keras.layers.LSTM(layer_0_units, input_shape = (seq_len,n_neurons), dropout=drop_in, recurrent_dropout=drop_rec))\n",
    "    mdl.add(tf.keras.layers.Dropout(drop_lay))\n",
    "    mdl.add(tf.keras.layers.Dense(rnn_train_o.shape[1]))\n",
    "\n",
    "    mdl.compile(loss=vrex_loss, optimizer='rmsprop', metrics='mse')\n",
    "                \n",
    "    mdl.fit(rnn_train_i, rnn_train_o, epochs=30, batch_size=128, verbose=1, callbacks=tensorboard_callback)\n",
    "    \n",
    "    \n",
    "    \n",
    "    train_preds = {}\n",
    "    train_VAFs = {}\n",
    "    for in_name in train_firing.keys():\n",
    "#         # create the array for training:\n",
    "#         temp_train_i = np.ndarray((train_firing[in_name].shape[0], seq_len, n_neurons))\n",
    "#         for ii in np.arange(seq_len):\n",
    "#             temp_train_i[:,ii,:] = train_firing[in_name].shift(-ii, fill_value=0).to_numpy()\n",
    "            \n",
    "#         # testing array -- since we have to add those extra columns\n",
    "#         temp_train_o = train_EMG[in_name].to_numpy()\n",
    "#         print(temp_train_o.shape)\n",
    "#         temp_train_o = np.append(temp_train_o, np.zeros((temp_train_o.shape[0],1)),axis=1)\n",
    "        \n",
    "        # predict\n",
    "        train_preds[in_name] = mdl.predict(train_i[in_name]) # for each training condition\n",
    "        train_VAFs[in_name] = metrics.r2_score(train_o[in_name][:,:n_target], train_preds[in_name][:,:n_target], multioutput='raw_values')\n",
    "    \n",
    "    \n",
    "    # and for each of our test conditions\n",
    "    test_preds = {}\n",
    "    test_VAFs = {}\n",
    "    for out_name in test_firing.keys():\n",
    "        # create the array for training:\n",
    "        temp_test_i = np.ndarray((test_firing[out_name].shape[0], seq_len, n_neurons))\n",
    "        for ii in np.arange(seq_len):\n",
    "            temp_test_i[:,ii,:] = test_firing[out_name].shift(-ii, fill_value=0).to_numpy()\n",
    "            \n",
    "        # testing array -- since we have to add those extra columns\n",
    "        temp_test_o = test_EMG[out_name].to_numpy()\n",
    "        temp_test_o = np.append(temp_test_o, np.zeros((temp_test_o.shape[0],n_target+1)),axis=1)\n",
    "        \n",
    "        # predict\n",
    "        test_preds[out_name] = mdl.predict(temp_test_i) # for each training condition\n",
    "        test_VAFs[out_name] = metrics.r2_score(temp_test_o[:,:n_target], test_preds[out_name][:,:n_target], multioutput='raw_values')\n",
    "    \n",
    "    \n",
    "    train_preds['Combined'] = mdl.predict(rnn_train_i)\n",
    "    train_VAFs['Combined'] = metrics.r2_score(rnn_train_o[:,:n_target], train_preds['Combined'][:,:n_target], multioutput='raw_values')\n",
    "\n",
    "    \n",
    "    print('----------------------------------------')\n",
    "    for ii,musc in enumerate(col_names):\n",
    "        print(musc)\n",
    "        for in_name,in_vaf in train_VAFs.items():\n",
    "            print(f\"\\tTrain VAF for {in_name}: {in_vaf[ii]}\")\n",
    "        for out_name,out_vaf in test_VAFs.items():\n",
    "            print(f\"\\tTest VAF for {out_name}: {out_vaf[ii]}\")\n",
    "    \n",
    "    \n",
    "#     # plottin\n",
    "    for train_name, train_data in train_preds.items(): \n",
    "        if train_name == 'Combined':\n",
    "            continue\n",
    "\n",
    "        n_rows = int(np.ceil(np.sqrt(n_target)))\n",
    "        n_cols = int(n_target//n_rows)\n",
    "        fig_lstm, ax_lstm = plt.subplots(nrows=n_rows, ncols=n_cols, sharex=True, constrained_layout=True)\n",
    "\n",
    "        for muscle_ii in np.arange(n_target):\n",
    "            row_i = int(muscle_ii//n_rows)\n",
    "            col_i = int(muscle_ii%n_rows)\n",
    "            ax_lstm[row_i,col_i].plot(train_EMG[train_name].to_numpy()[:,muscle_ii], label='Recorded')\n",
    "            ax_lstm[row_i,col_i].plot(train_data[:,muscle_ii], label=f'LSTM VAF: {train_VAFs[train_name][muscle_ii]:.03f}')\n",
    "            ax_lstm[row_i,col_i].set_title(f\"{col_names[muscle_ii]}\")\n",
    "            _ = ax_lstm[row_i,col_i].legend()\n",
    "\n",
    "            # turn off the spines\n",
    "            for spine in ['right','top','bottom','left']:\n",
    "                ax_lstm[row_i,col_i].spines[spine].set_visible(False)\n",
    "        \n",
    "        fig_lstm.suptitle(train_name)\n",
    "    \n",
    "    \n",
    "    plot_VAFs(train_VAFs, test_VAFs, col_names)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_rex_weighted(train_firing: dict, train_EMG: dict, test_firing: dict, test_EMG: dict, test_timestamps, plot=True):\n",
    "#     if type(train_firing) is not list:\n",
    "#         print(f\"train_firing must be a list of Numpy Arrays, not a {type(train_firing)}\")\n",
    "#         return -1\n",
    "    if type(train_firing) is not dict:\n",
    "        print(f\"train firing must be a dict of Numpy Arrays, not a {type(train_firing)}\")\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "    # hyper params\n",
    "    layer_0_units = 300\n",
    "    drop_in = .25     # input dropout percentage for LSTM layer\n",
    "    drop_rec = 0    # recurrent dropout for LSTM\n",
    "    drop_lay = .15    # dropout layer?\n",
    "    \n",
    "    # size and name of outputs\n",
    "    train_sets = list(train_EMG.keys())\n",
    "    n_target = train_EMG[train_sets[0]].shape[1] # dimensionality of target (EMGs usually)\n",
    "    col_names = train_EMG[train_sets[0]].columns\n",
    "    \n",
    "\n",
    "    # tensorboard callback\n",
    "    log_dir = f'C:\\\\Users\\\\17204\\\\Documents\\\\Tensorboard_logs\\\\LSTM_logs\\\\{dt.today().strftime(\"%Y%m%d\")}_{\"REx_weighted\"}'\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    \n",
    "\n",
    "    # start by initializing the input sequence data\n",
    "    n_neurons = train_firing[train_sets[0]].shape[1]\n",
    "    seq_len = 10\n",
    "    rnn_train_i = np.ndarray((0,seq_len, n_neurons))\n",
    "    \n",
    "    # training inputs\n",
    "    for train_name,train_data in train_firing.items():\n",
    "        temp_train_i = np.ndarray((train_data.shape[0], seq_len, n_neurons))\n",
    "        for ii in np.arange(seq_len):\n",
    "            temp_train_i[:,ii,:] = train_data.shift(-ii, fill_value=0).to_numpy()\n",
    "        rnn_train_i = np.append(rnn_train_i, temp_train_i, axis=0)\n",
    "            \n",
    "#     for ii in np.arange(seq_len):\n",
    "#         rnn_test_i[:,ii,:] = test_firing.shift(-ii, fill_value=0).to_numpy()\n",
    "        \n",
    "\n",
    "#     rnn_test_o = test_EMG.to_numpy() # don't need the categories for the test set\n",
    "    rnn_train_o = np.ndarray((0,2*n_target+1)) # initialize the training outputs\n",
    "    for ii_EMG, EMG in enumerate(train_EMG.values()): # iterate through list\n",
    "        # put together the weights -- the variances, since we're doing MSE as our training loss\n",
    "        temp_var = np.matmul(np.ones((EMG.shape[0],1)),np.var(EMG.to_numpy(), axis=0).reshape(1,-1))\n",
    "#         loss_weights = np.append(loss_weights,temp_var, axis=0)\n",
    "        # append the training targets\n",
    "        temp_train = np.append(EMG.to_numpy(), 1./temp_var, axis=1)\n",
    "        temp_train = np.append(temp_train, ii_EMG*np.ones((EMG.shape[0],1)), axis=-1)\n",
    "        rnn_train_o = np.append(rnn_train_o, temp_train, axis=0)\n",
    "        \n",
    "#         temp_train = EMG.to_numpy() # add the numpy stuff\n",
    "#         temp_train = np.append(temp_train, np.ones([temp_train.shape[0],1])*(1/np.var(temp_train, axis=0, keepdims=True)), axis=-1) # append a category flag to the last column\n",
    "#         temp_train = np.append(temp_train, ii_EMG*np.ones([temp_train.shape[0],1]), axis=-1)\n",
    "#         rnn_train_o = np.append(rnn_train_o, temp_train, axis=0)\n",
    "        \n",
    "\n",
    "    # normalize EMGs -- shouldn't be necessary in this case :)\n",
    "    # EMG_std = np.std(rnn_train_o, axis=0)\n",
    "    # for ii in np.arange(rnn_train_o.shape[1] - 1):\n",
    "    #     rnn_train_o[:,ii] = rnn_train_o[:,ii]/EMG_std[ii]\n",
    "    #     rnn_test_o[:,ii] = rnn_test_o[:,ii]/EMG_std[ii]\n",
    "\n",
    "\n",
    "    # create the model\n",
    "    mdl = tf.keras.models.Sequential()\n",
    "\n",
    "    # add the LSTM layer\n",
    "    mdl.add(tf.keras.layers.LSTM(layer_0_units, input_shape = (seq_len,n_neurons), dropout=drop_in, recurrent_dropout=drop_rec))\n",
    "    mdl.add(tf.keras.layers.Dropout(drop_lay))\n",
    "    mdl.add(tf.keras.layers.Dense(rnn_train_o.shape[1]))\n",
    "\n",
    "    mdl.compile(loss=vrex_weighted, optimizer='rmsprop', metrics='mse')\n",
    "                \n",
    "    mdl.fit(rnn_train_i, rnn_train_o, epochs=30, batch_size=128, verbose=1, callbacks=tensorboard_callback)\n",
    "    \n",
    "    # -----------------------------------------------------\n",
    "    # Testing out the predictions, getting R2 scores etc\n",
    "    \n",
    "    # now let's create separate VAFs for each of the training conditions:\n",
    "    train_preds = {}\n",
    "    train_VAFs = {}\n",
    "    for in_name in train_firing.keys():\n",
    "        # create the array for training:\n",
    "        temp_train_i = np.ndarray((train_firing[in_name].shape[0], seq_len, n_neurons))\n",
    "        for ii in np.arange(seq_len):\n",
    "            temp_train_i[:,ii,:] = train_firing[in_name].shift(-ii, fill_value=0).to_numpy()\n",
    "            \n",
    "        # testing array -- since we have to add those extra columns\n",
    "        temp_train_o = train_EMG[in_name].to_numpy()\n",
    "        print(temp_train_o.shape)\n",
    "        temp_train_o = np.append(temp_train_o, np.zeros((temp_train_o.shape[0],n_target+1)),axis=1)\n",
    "        \n",
    "        # predict\n",
    "        train_preds[in_name] = mdl.predict(temp_train_i) # for each training condition\n",
    "        train_VAFs[in_name] = metrics.r2_score(temp_train_o[:,:n_target], train_preds[in_name][:,:n_target], multioutput='raw_values')\n",
    "    \n",
    "    \n",
    "    # and for each of our test conditions\n",
    "    test_preds = {}\n",
    "    test_VAFs = {}\n",
    "    for out_name in test_firing.keys():\n",
    "        # create the array for training:\n",
    "        temp_test_i = np.ndarray((test_firing[out_name].shape[0], seq_len, n_neurons))\n",
    "        for ii in np.arange(seq_len):\n",
    "            temp_test_i[:,ii,:] = test_firing[out_name].shift(-ii, fill_value=0).to_numpy()\n",
    "            \n",
    "        # testing array -- since we have to add those extra columns\n",
    "        temp_test_o = test_EMG[out_name].to_numpy()\n",
    "        temp_test_o = np.append(temp_test_o, np.zeros((temp_test_o.shape[0],n_target+1)),axis=1)\n",
    "        \n",
    "        # predict\n",
    "        test_preds[out_name] = mdl.predict(temp_test_i) # for each training condition\n",
    "        test_VAFs[out_name] = metrics.r2_score(temp_test_o[:,:n_target], test_preds[out_name][:,:n_target], multioutput='raw_values')\n",
    "    \n",
    "    \n",
    "    train_preds['Combined'] = mdl.predict(rnn_train_i)\n",
    "    train_VAFs['Combined'] = metrics.r2_score(rnn_train_o[:,:n_target], train_preds['Combined'][:,:n_target], multioutput='raw_values')\n",
    "\n",
    "    \n",
    "    print('----------------------------------------')\n",
    "    for ii,musc in enumerate(col_names):\n",
    "        print(musc)\n",
    "        for in_name,in_vaf in train_VAFs.items():\n",
    "            print(f\"\\tTrain VAF for {in_name}: {in_vaf[ii]}\")\n",
    "        for out_name,out_vaf in test_VAFs.items():\n",
    "            print(f\"\\tTest VAF for {out_name}: {out_vaf[ii]}\")\n",
    "    \n",
    "    \n",
    "#     # plottin\n",
    "    for train_name, train_data in train_preds.items(): \n",
    "        if train_name == 'Combined':\n",
    "            continue\n",
    "\n",
    "        n_rows = int(np.ceil(np.sqrt(n_target)))\n",
    "        n_cols = int(n_target//n_rows)\n",
    "        fig_lstm, ax_lstm = plt.subplots(nrows=n_rows, ncols=n_cols, sharex=True, constrained_layout=True)\n",
    "\n",
    "        for muscle_ii in np.arange(n_target):\n",
    "            row_i = int(muscle_ii//n_rows)\n",
    "            col_i = int(muscle_ii%n_rows)\n",
    "            ax_lstm[row_i,col_i].plot(train_EMG[train_name].to_numpy()[:,muscle_ii], label='Recorded')\n",
    "            ax_lstm[row_i,col_i].plot(train_data[:,muscle_ii], label=f'LSTM VAF: {train_VAFs[train_name][muscle_ii]:.03f}')\n",
    "            ax_lstm[row_i,col_i].set_title(f\"{col_names[muscle_ii]}\")\n",
    "            _ = ax_lstm[row_i,col_i].legend()\n",
    "\n",
    "            # turn off the spines\n",
    "            for spine in ['right','top','bottom','left']:\n",
    "                ax_lstm[row_i,col_i].spines[spine].set_visible(False)\n",
    "        \n",
    "        fig_lstm.suptitle(train_name)\n",
    "    \n",
    "    \n",
    "    plot_VAFs(train_VAFs, test_VAFs, col_names)\n",
    "    \n",
    "#     # Plotting VAFs -- comparing the training sets and testing sets\n",
    "#     fig_vaf, ax_vaf = plt.subplots(ncols = 2, constrained_layout=True)\n",
    "#     i_muscles = np.arange(n_target) # indexing on the x axis\n",
    "#     n_train = len(train_VAFs)\n",
    "#     n_test = len(test_VAFs)\n",
    "#     train_width = (8//n_train)*.1 # the bars together should only take 80% of each muscle's width\n",
    "#     test_width = (8//n_test)*.1\n",
    "    \n",
    "#     # training plot\n",
    "#     for train_ii, vaf in enumerate(train_VAFs.items()):\n",
    "#         ax_vaf[0].bar(i_muscles + train_width*train_ii, vaf[1], width=train_width, label=vaf[0])\n",
    "#     ax_vaf[0].set_xticks(i_muscles + .4)\n",
    "#     ax_vaf[0].set_xticklabels(col_names)\n",
    "#     ax_vaf[0].set_title('Training datasets')\n",
    "    \n",
    "#     # test plot\n",
    "#     for test_ii, vaf in enumerate(test_VAFs.items()):                              \n",
    "#         ax_vaf[1].bar(i_muscles + test_width*test_ii, vaf[1], width = test_width, label=vaf[0])\n",
    "#     ax_vaf[1].set_xticks(i_muscles + .4)\n",
    "#     ax_vaf[1].set_xticklabels(col_names)\n",
    "#     ax_vaf[1].set_title('Testing Datasets')\n",
    "\n",
    "\n",
    "\n",
    "#     # For each bar in the chart, add a text label.\n",
    "#     for chart in ax_vaf:\n",
    "#         chart.set_ylim([-.05, 1.05])\n",
    "#         chart.set_xlabel('Muscle')\n",
    "#         chart.set_ylabel('R2 Score')\n",
    "\n",
    "#         chart.legend()\n",
    "\n",
    "#         for bar in chart.patches:\n",
    "#             # The text annotation for each bar should be its height.\n",
    "#             bar_value = bar.get_height()\n",
    "#             # Format the text with commas to separate thousands. You can do\n",
    "#             # any type of formatting here though.\n",
    "#             text = f'{bar_value:.02f}'\n",
    "#             # This will give the middle of each bar on the x-axis.\n",
    "#             text_x = bar.get_x() + bar.get_width() / 2\n",
    "#             # get_y() is where the bar starts so we add the height to it.\n",
    "#             text_y = np.max([bar.get_y() + bar_value, 0.01]) # keep it from going too far below zero, and disappearing\n",
    "#             # If we want the text to be the same color as the bar, we can\n",
    "#             # get the color like so:\n",
    "#             bar_color = bar.get_facecolor()\n",
    "#             # If you want a consistent color, you can just set it as a constant, e.g. #222222\n",
    "#             chart.text(text_x, text_y, text, ha='center', va='bottom', color=bar_color,\n",
    "#                     size=12)\n",
    "\n",
    "#         # turn off the spines\n",
    "#         for spine in ['right','top','bottom','left']:\n",
    "#             chart.spines[spine].set_visible(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different combinations of train/test sets\n",
    "\n",
    "So that we can quickly run through all of the iterations\n",
    "\n",
    "Set the nonlinearity type, to compare across iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonlinear_type = 'poly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "First train wrist movement, test wrist movement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_timestamps, train_firing, train_EMG, train_force, train_kin = load_josh_mat(data['WmTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['WmTest'])\n",
    "# using the training set twice as the \"refit\" gain -- for sanity's sake\n",
    "mov_mov_linVAF = basic_decoder_comparison(train_firing, train_EMG, train_firing, train_EMG, test_firing, test_EMG, test_timestamps, nonlinear_type)\n",
    "mov_mov_LSTMVAF = LSTM_comparisons(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "print(f\"Mean Linear {np.mean(mov_mov_linVAF)}\")\n",
    "print(f\"Mean LSTM {np.mean(mov_mov_LSTMVAF)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train wrist movement, test iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_timestamps, train_firing, train_EMG, train_force, train_kin = load_josh_mat(data['WmTrain'])\n",
    "_, retrain_firing, retrain_EMG, _, _, = load_josh_mat(data['IsoTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['IsoTest'])\n",
    "mov_iso_linVAF = basic_decoder_comparison(train_firing, train_EMG, retrain_firing, retrain_EMG, test_firing, test_EMG, test_timestamps, nonlinear_type)\n",
    "mov_iso_LSTMVAF = LSTM_comparisons(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "print(f\"Mean Linear {np.mean(mov_iso_linVAF)}\")\n",
    "print(f\"Mean LSTM {np.mean(mov_iso_LSTMVAF)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train WM, test spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_timestamps, train_firing, train_EMG, train_force, train_kin = load_josh_mat(data['WmTrain'])\n",
    "_, retrain_firing, retrain_EMG, _, _, = load_josh_mat(data['SprTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['SprTest'])\n",
    "mov_spr_linVAF = basic_decoder_comparison(train_firing, train_EMG, retrain_firing, retrain_EMG, test_firing, test_EMG, test_timestamps, nonlinear_type)\n",
    "mov_spr_LSTMVAF = LSTM_comparisons(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "print(f\"Mean Linear {np.mean(mov_spr_linVAF)}\")\n",
    "print(f\"Mean LSTM {np.mean(mov_spr_LSTMVAF)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Iso, test Iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_timestamps, train_firing, train_EMG, train_force, train_kin = load_josh_mat(data['IsoTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['IsoTest'])\n",
    "iso_iso_linVAF = basic_decoder_comparison(train_firing, train_EMG, train_firing, train_EMG, test_firing, test_EMG, test_timestamps, nonlinear_type)\n",
    "iso_iso_LSTMVAF = LSTM_comparisons(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "print(f\"Mean Linear {np.mean(iso_iso_linVAF)}\")\n",
    "print(f\"Mean LSTM {np.mean(iso_iso_LSTMVAF)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Iso, test WM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_timestamps, train_firing, train_EMG, train_force, train_kin = load_josh_mat(data['IsoTrain'])\n",
    "_, retrain_firing, retrain_EMG, _, _, = load_josh_mat(data['WmTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['WmTest'])\n",
    "iso_mov_linVAF = basic_decoder_comparison(train_firing, train_EMG, retrain_firing, retrain_EMG, test_firing, test_EMG, test_timestamps, nonlinear_type)\n",
    "iso_mov_LSTMVAF = LSTM_comparisons(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "print(f\"Mean Linear {np.mean(iso_mov_linVAF)}\")\n",
    "print(f\"Mean LSTM {np.mean(iso_mov_linVAF)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Iso, Test Spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_timestamps, train_firing, train_EMG, train_force, train_kin = load_josh_mat(data['IsoTrain'])\n",
    "_, retrain_firing, retrain_EMG, _, _, = load_josh_mat(data['SprTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['SprTest'])\n",
    "iso_spr_linVAF = basic_decoder_comparison(train_firing, train_EMG, retrain_firing, retrain_EMG, test_firing, test_EMG, test_timestamps, nonlinear_type)\n",
    "iso_spr_LSTMVAF = LSTM_comparisons(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "print(f\"Mean Linear {np.mean(iso_spr_linVAF)}\")\n",
    "print(f\"Mean LSTM {np.mean(iso_spr_linVAF)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Spring, test each of the three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_timestamps, train_firing, train_EMG, train_force, train_kin = load_josh_mat(data['SprTrain'])\n",
    "\n",
    "print('Test Spring')\n",
    "_, retrain_firing, retrain_EMG, _, _, = load_josh_mat(data['SprTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['SprTest'])\n",
    "spr_spr_linVAF = basic_decoder_comparison(train_firing, train_EMG, retrain_firing, retrain_EMG, test_firing, test_EMG, test_timestamps, nonlinear_type)\n",
    "spr_spr_LSTMVAF = LSTM_comparisons(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "print('Test Iso')\n",
    "_, retrain_firing, retrain_EMG, _, _, = load_josh_mat(data['IsoTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['IsoTest'])\n",
    "spr_iso_linVAF = basic_decoder_comparison(train_firing, train_EMG, retrain_firing, retrain_EMG, test_firing, test_EMG, test_timestamps, nonlinear_type)\n",
    "spr_iso_LSTMVAF = LSTM_comparisons(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "print('Test Movement')\n",
    "_, retrain_firing, retrain_EMG, _, _, = load_josh_mat(data['WmTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['WmTest'])\n",
    "spr_mov_linVAF = basic_decoder_comparison(train_firing, train_EMG, retrain_firing, retrain_EMG, test_firing, test_EMG, test_timestamps, nonlinear_type)\n",
    "spr_mov_LSTMVAF = LSTM_comparisons(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "dntxt.send('Spring Linear and non weighted done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train on hybrid, test on each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hybrid 3 -- this should be a blend of all three, I think for training\n",
    "train_timestamps, train_firing, train_EMG, train_force, train_kin = load_josh_mat(data['Hybrid3'])\n",
    "\n",
    "print(\"test on movement\")\n",
    "_, retrain_firing, retrain_EMG, _, _, = load_josh_mat(data['WmTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['WmTest'])\n",
    "basic_decoder_comparison(train_firing, train_EMG, retrain_firing, retrain_EMG, test_firing, test_EMG, test_timestamps, nonlinear_type)\n",
    "LSTM_comparisons(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "print(\"\\n\\ntest on Iso\")\n",
    "_, retrain_firing, retrain_EMG, _, _, = load_josh_mat(data['IsoTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['IsoTest'])\n",
    "basic_decoder_comparison(train_firing, train_EMG, retrain_firing, retrain_EMG, test_firing, test_EMG, test_timestamps, nonlinear_type)\n",
    "LSTM_comparisons(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "\n",
    "print(\"\\n\\ntest on Spring\")\n",
    "_, retrain_firing, retrain_EMG, _, _, = load_josh_mat(data['SprTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['SprTest'])\n",
    "basic_decoder_comparison(train_firing, train_EMG, retrain_firing, retrain_EMG, test_firing, test_EMG, test_timestamps, nonlinear_type)\n",
    "LSTM_comparisons(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "dntxt.send('Hybrid basic testing complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hybrid 3 -- this should be a blend of all three, I think for training\n",
    "train_timestamps, train_firing, train_EMG, train_force, train_kin = load_josh_mat(data['Hybrid3'])\n",
    "_, retrain_firing, retrain_EMG, _, _, = load_josh_mat(data['IsoTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['IsoTest'])\n",
    "basic_decoder_comparison(train_firing, train_EMG, retrain_firing, retrain_EMG, test_firing, test_EMG, test_timestamps, nonlinear_type)\n",
    "LSTM_comparisons(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid LSTMs with weighted loss functions\n",
    "\n",
    "This works a little different from Josh's -- I'm not basing it off of times; instead, I'm manually compiling the train sets I want to use\n",
    "\n",
    "\n",
    "Train on Iso and Movement, test on each of the three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, train_firing_0, train_EMG_0, _, _ = load_josh_mat(data['IsoTrain'])\n",
    "_, train_firing_1, train_EMG_1, _, _ = load_josh_mat(data['WmTrain'])\n",
    "\n",
    "print(\"test on movement\")\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['WmTest'])\n",
    "LSTM_across_weighted([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "print('test on iso')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['IsoTest'])\n",
    "LSTM_across_weighted([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "print('test on Spring')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['SprTest'])\n",
    "LSTM_across_weighted([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on Movement and Spring, test on everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_firing_0, train_EMG_0, _, _ = load_josh_mat(data['SprTrain'])\n",
    "_, train_firing_1, train_EMG_1, _, _ = load_josh_mat(data['WmTrain'])\n",
    "\n",
    "print(f\"Test on Spring\")\n",
    "test_timestamps, test_firing, test_EMG, _, _ = load_josh_mat(data['SprTest'])\n",
    "LSTM_across_weighted([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps, plot=False)\n",
    "\n",
    "print(f\"Test on Movement\")\n",
    "test_timestamps, test_firing, test_EMG, _, _ = load_josh_mat(data['WmTest'])\n",
    "LSTM_across_weighted([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps, plot=False)\n",
    "\n",
    "print(f\"Test on Iso\")\n",
    "test_timestamps, test_firing, test_EMG, _, _ = load_josh_mat(data['IsoTest'])\n",
    "LSTM_across_weighted([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on Spring and Iso, test on everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, train_firing_0, train_EMG_0, _, _ = load_josh_mat(data['SprTrain'])\n",
    "_, train_firing_1, train_EMG_1, _, _ = load_josh_mat(data['IsoTrain'])\n",
    "\n",
    "print(f\"Test on Spring\")\n",
    "test_timestamps, test_firing, test_EMG, _, _ = load_josh_mat(data['SprTest'])\n",
    "LSTM_across_weighted([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "print(f\"Test on Movement\")\n",
    "test_timestamps, test_firing, test_EMG, _, _ = load_josh_mat(data['WmTest'])\n",
    "LSTM_across_weighted([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "print(f\"Test on Iso\")\n",
    "test_timestamps, test_firing, test_EMG, _, _ = load_josh_mat(data['IsoTest'])\n",
    "LSTM_across_weighted([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train on everything, test on each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_firing_0, train_EMG_0, _, _ = load_josh_mat(data['SprTrain'])\n",
    "_, train_firing_1, train_EMG_1, _, _ = load_josh_mat(data['IsoTrain'])\n",
    "_, train_firing_2, train_EMG_2, _, _ = load_josh_mat(data['IsoTrain'])\n",
    "\n",
    "print(f\"Test on Spring\")\n",
    "test_timestamps, test_firing, test_EMG, _, _ = load_josh_mat(data['SprTest'])\n",
    "LSTM_across_weighted([train_firing_0, train_firing_1, train_firing_2], [train_EMG_0, train_EMG_1, train_EMG_2], test_firing, test_EMG, test_timestamps, plot=False)\n",
    "\n",
    "print(f\"Test on Movement\")\n",
    "test_timestamps, test_firing, test_EMG, _, _ = load_josh_mat(data['WmTest'])\n",
    "LSTM_across_weighted([train_firing_0, train_firing_1, train_firing_2], [train_EMG_0, train_EMG_1, train_EMG_2], test_firing, test_EMG, test_timestamps, plot=False)\n",
    "\n",
    "print(f\"Test on Iso\")\n",
    "test_timestamps, test_firing, test_EMG, _, _ = load_josh_mat(data['IsoTest'])\n",
    "LSTM_across_weighted([train_firing_0, train_firing_1, train_firing_2], [train_EMG_0, train_EMG_1, train_EMG_2], test_firing, test_EMG, test_timestamps, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM grid search\n",
    "\n",
    "Time to look at some non-linearities!\n",
    "\n",
    "Let's run through the grid search on hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, firing, EMG, _, _ = load_josh_mat(data['WmTrain'])\n",
    "\n",
    "log = LSTM_grid_search(firing, EMG, n_iter = 30, n_fold = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM with REx\n",
    "\n",
    "Run through the options, see what comes out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Movement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-26 13:44:54.285939: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 13:44:54.286388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-10-26 13:44:54.286835: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-26 13:44:54.286887: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-26 13:44:54.286935: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-26 13:44:54.286982: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-10-26 13:44:54.287029: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-10-26 13:44:54.287076: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-26 13:44:54.287122: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-10-26 13:44:54.287170: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-10-26 13:44:54.287176: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-10-26 13:44:54.287496: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "94/94 [==============================] - 4s 22ms/step - loss: 0.0616 - mse: 0.3598\n",
      "Epoch 2/30\n",
      "94/94 [==============================] - 2s 23ms/step - loss: 0.0040 - mse: 0.2495\n",
      "Epoch 3/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0029 - mse: 0.2814\n",
      "Epoch 4/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0027 - mse: 0.2614\n",
      "Epoch 5/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0027 - mse: 0.2361\n",
      "Epoch 6/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0026 - mse: 0.2237\n",
      "Epoch 7/30\n",
      "94/94 [==============================] - 2s 23ms/step - loss: 0.0025 - mse: 0.2003\n",
      "Epoch 8/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0025 - mse: 0.1884\n",
      "Epoch 9/30\n",
      "94/94 [==============================] - 2s 23ms/step - loss: 0.0024 - mse: 0.1778\n",
      "Epoch 10/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0024 - mse: 0.1726\n",
      "Epoch 11/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0024 - mse: 0.1708\n",
      "Epoch 12/30\n",
      "94/94 [==============================] - 2s 23ms/step - loss: 0.0023 - mse: 0.1710\n",
      "Epoch 13/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0023 - mse: 0.1709\n",
      "Epoch 14/30\n",
      "94/94 [==============================] - 2s 23ms/step - loss: 0.0023 - mse: 0.1792\n",
      "Epoch 15/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0023 - mse: 0.1708\n",
      "Epoch 16/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0023 - mse: 0.1681\n",
      "Epoch 17/30\n",
      "94/94 [==============================] - 2s 23ms/step - loss: 0.0022 - mse: 0.1696\n",
      "Epoch 18/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0022 - mse: 0.1631\n",
      "Epoch 19/30\n",
      "94/94 [==============================] - 2s 23ms/step - loss: 0.0022 - mse: 0.1702\n",
      "Epoch 20/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0022 - mse: 0.1632\n",
      "Epoch 21/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0022 - mse: 0.1715\n",
      "Epoch 22/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0021 - mse: 0.1696\n",
      "Epoch 23/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0021 - mse: 0.1642\n",
      "Epoch 24/30\n",
      "94/94 [==============================] - 2s 23ms/step - loss: 0.0021 - mse: 0.1713\n",
      "Epoch 25/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0021 - mse: 0.1637\n",
      "Epoch 26/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0021 - mse: 0.1700\n",
      "Epoch 27/30\n",
      "94/94 [==============================] - 2s 23ms/step - loss: 0.0021 - mse: 0.1621\n",
      "Epoch 28/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0021 - mse: 0.1696\n",
      "Epoch 29/30\n",
      "94/94 [==============================] - 2s 23ms/step - loss: 0.0021 - mse: 0.1656\n",
      "Epoch 30/30\n",
      "94/94 [==============================] - 2s 22ms/step - loss: 0.0020 - mse: 0.1565\n",
      "375/375 [==============================] - 2s 4ms/step\n",
      "188/188 [==============================] - 1s 4ms/step\n",
      "188/188 [==============================] - 1s 4ms/step\n",
      "188/188 [==============================] - 1s 4ms/step\n",
      "375/375 [==============================] - 1s 4ms/step\n",
      "----------------------------------------\n",
      "FCU\n",
      "\tTrain VAF for Move: 0.1071886300637438\n",
      "\tTrain VAF for Combined: 0.1071886300637479\n",
      "\tTest VAF for Move: 0.11180691439359114\n",
      "\tTest VAF for Iso: 0.04100986222815317\n",
      "\tTest VAF for Spr: 0.031727140351017535\n",
      "FCR\n",
      "\tTrain VAF for Move: 0.358236390217336\n",
      "\tTrain VAF for Combined: 0.35823639021733766\n",
      "\tTest VAF for Move: 0.20666577579437806\n",
      "\tTest VAF for Iso: -0.1189004707460255\n",
      "\tTest VAF for Spr: -0.13915526179422377\n",
      "ECU\n",
      "\tTrain VAF for Move: 0.6123517778933079\n",
      "\tTrain VAF for Combined: 0.6123517778933073\n",
      "\tTest VAF for Move: 0.39109168287741614\n",
      "\tTest VAF for Iso: -1.614373012566841\n",
      "\tTest VAF for Spr: 0.1821493449748517\n",
      "ECR\n",
      "\tTrain VAF for Move: 0.6404976677964145\n",
      "\tTrain VAF for Combined: 0.6404976677964143\n",
      "\tTest VAF for Move: 0.5192402389317861\n",
      "\tTest VAF for Iso: -0.05509833809436637\n",
      "\tTest VAF for Spr: 0.2772255167140555\n"
     ]
    }
   ],
   "source": [
    "print('Training on Movement')\n",
    "\n",
    "_, train_firing_1, train_EMG_1, _, _ = load_josh_mat(data['WmTrain'])\n",
    "\n",
    "train_firing = {'Move': train_firing_1}\n",
    "train_EMG = {'Move': train_EMG_1}\n",
    "\n",
    "# testing dictionaries\n",
    "test_firing = {}\n",
    "test_EMG = {}\n",
    "test_timestamps = {}\n",
    "# movement\n",
    "test_timestamps['Move'], test_firing['Move'], test_EMG['Move'], _, _ = load_josh_mat(data['WmTest'])\n",
    "test_timestamps['Iso'], test_firing['Iso'], test_EMG['Iso'], _, _ = load_josh_mat(data['IsoTest'])\n",
    "test_timestamps['Spr'], test_firing['Spr'], test_EMG['Spr'], _, _ = load_josh_mat(data['SprTest'])\n",
    "\n",
    "# Send it\n",
    "LSTM_rex(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on isometric and movement\n",
      "Epoch 1/30\n",
      "188/188 [==============================] - 6s 22ms/step - loss: 0.1510 - mse: 0.5411\n",
      "Epoch 2/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0136 - mse: 0.4834\n",
      "Epoch 3/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0129 - mse: 0.4976\n",
      "Epoch 4/30\n",
      "188/188 [==============================] - 4s 22ms/step - loss: 0.0126 - mse: 0.5206\n",
      "Epoch 5/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0125 - mse: 0.5304\n",
      "Epoch 6/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0123 - mse: 0.5382\n",
      "Epoch 7/30\n",
      "188/188 [==============================] - 4s 22ms/step - loss: 0.0121 - mse: 0.5431\n",
      "Epoch 8/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0120 - mse: 0.5479\n",
      "Epoch 9/30\n",
      "188/188 [==============================] - 4s 22ms/step - loss: 0.0119 - mse: 0.5558\n",
      "Epoch 10/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0118 - mse: 0.5689\n",
      "Epoch 11/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0118 - mse: 0.5728\n",
      "Epoch 12/30\n",
      "188/188 [==============================] - 4s 22ms/step - loss: 0.0117 - mse: 0.5600\n",
      "Epoch 13/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0115 - mse: 0.5672\n",
      "Epoch 14/30\n",
      "188/188 [==============================] - 4s 22ms/step - loss: 0.0115 - mse: 0.5745\n",
      "Epoch 15/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0114 - mse: 0.5728\n",
      "Epoch 16/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0115 - mse: 0.5802\n",
      "Epoch 17/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0113 - mse: 0.5849\n",
      "Epoch 18/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0113 - mse: 0.5678\n",
      "Epoch 19/30\n",
      "188/188 [==============================] - 4s 22ms/step - loss: 0.0112 - mse: 0.5774\n",
      "Epoch 20/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0112 - mse: 0.5614\n",
      "Epoch 21/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0111 - mse: 0.5812\n",
      "Epoch 22/30\n",
      "188/188 [==============================] - 4s 22ms/step - loss: 0.0111 - mse: 0.6122\n",
      "Epoch 23/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0110 - mse: 0.5969\n",
      "Epoch 24/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0110 - mse: 0.5958\n",
      "Epoch 25/30\n",
      "188/188 [==============================] - 4s 22ms/step - loss: 0.0110 - mse: 0.5962\n",
      "Epoch 26/30\n",
      "188/188 [==============================] - 4s 22ms/step - loss: 0.0108 - mse: 0.6107\n",
      "Epoch 27/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0109 - mse: 0.6189\n",
      "Epoch 28/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0108 - mse: 0.6259\n",
      "Epoch 29/30\n",
      "188/188 [==============================] - 4s 23ms/step - loss: 0.0108 - mse: 0.6300\n",
      "Epoch 30/30\n",
      "188/188 [==============================] - 4s 22ms/step - loss: 0.0107 - mse: 0.6334\n",
      "375/375 [==============================] - 2s 4ms/step\n",
      "375/375 [==============================] - 1s 4ms/step\n",
      "188/188 [==============================] - 1s 4ms/step\n",
      "188/188 [==============================] - 1s 4ms/step\n",
      "188/188 [==============================] - 1s 4ms/step\n",
      "750/750 [==============================] - 3s 4ms/step\n",
      "----------------------------------------\n",
      "FCU\n",
      "\tTrain VAF for Iso: -0.012728227209819032\n",
      "\tTrain VAF for Move: 0.014198641414503776\n",
      "\tTrain VAF for Combined: 0.0007352071023497553\n",
      "\tTest VAF for Iso: -0.03077950085860448\n",
      "\tTest VAF for Move: -0.08084173583096166\n",
      "\tTest VAF for Spr: -0.1630807926407536\n",
      "FCR\n",
      "\tTrain VAF for Iso: 0.046976835575760933\n",
      "\tTrain VAF for Move: 0.29933844148781086\n",
      "\tTrain VAF for Combined: 0.1731576385317909\n",
      "\tTest VAF for Iso: -0.07232521361452182\n",
      "\tTest VAF for Move: 0.22034481916650484\n",
      "\tTest VAF for Spr: -0.06921614121618491\n",
      "ECU\n",
      "\tTrain VAF for Iso: 0.2706937740953458\n",
      "\tTrain VAF for Move: 0.5910458397686886\n",
      "\tTrain VAF for Combined: 0.43086980693202337\n",
      "\tTest VAF for Iso: -1.5093970418920422\n",
      "\tTest VAF for Move: 0.412774154630668\n",
      "\tTest VAF for Spr: 0.19005803855243208\n",
      "ECR\n",
      "\tTrain VAF for Iso: 0.20844205025929674\n",
      "\tTrain VAF for Move: 0.6230706392812096\n",
      "\tTrain VAF for Combined: 0.4157563447702666\n",
      "\tTest VAF for Iso: -0.12420455845289058\n",
      "\tTest VAF for Move: 0.5435355298613394\n",
      "\tTest VAF for Spr: 0.29342616999116855\n"
     ]
    }
   ],
   "source": [
    "print('Training on isometric and movement')\n",
    "\n",
    "_, train_firing_0, train_EMG_0, _, _ = load_josh_mat(data['IsoTrain'])\n",
    "_, train_firing_1, train_EMG_1, _, _ = load_josh_mat(data['WmTrain'])\n",
    "\n",
    "train_firing = {'Iso': train_firing_0, 'Move': train_firing_1}\n",
    "train_EMG = {'Iso': train_EMG_1, 'Move': train_EMG_1}\n",
    "\n",
    "# testing dictionaries\n",
    "test_firing = {}\n",
    "test_EMG = {}\n",
    "test_timestamps = {}\n",
    "\n",
    "# populate dictionaries\n",
    "test_timestamps['Iso'], test_firing['Iso'], test_EMG['Iso'], _, _ = load_josh_mat(data['IsoTest'])\n",
    "test_timestamps['Move'], test_firing['Move'], test_EMG['Move'], _, _ = load_josh_mat(data['WmTest'])\n",
    "test_timestamps['Spr'], test_firing['Spr'], test_EMG['Spr'], _, _ = load_josh_mat(data['SprTest'])\n",
    "\n",
    "# Send it\n",
    "LSTM_rex(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# doneTone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('movement and spring')\n",
    "\n",
    "\n",
    "_, train_firing_0, train_EMG_0, _, _ = load_josh_mat(data['SprTrain'])\n",
    "_, train_firing_1, train_EMG_1, _, _ = load_josh_mat(data['WmTrain'])\n",
    "\n",
    "print('\\nmovement test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['WmTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)\n",
    "print('\\niso test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['IsoTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)\n",
    "print('\\nspring test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['SprTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('iso and spring')\n",
    "\n",
    "\n",
    "_, train_firing_0, train_EMG_0, _, _ = load_josh_mat(data['IsoTrain'])\n",
    "_, train_firing_1, train_EMG_1, _, _ = load_josh_mat(data['SprTrain'])\n",
    "\n",
    "print('\\nmovement test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['WmTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)\n",
    "print('\\niso test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['IsoTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)\n",
    "print('\\nspring test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['SprTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_firing_0, train_EMG_0, _, _ = load_josh_mat(data['IsoTrain'])\n",
    "_, train_firing_1, train_EMG_1, _, _ = load_josh_mat(data['WmTrain'])\n",
    "_, train_firing_2, train_EMG_2, _, _ = load_josh_mat(data['SprTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['SprTest'])\n",
    "\n",
    "\n",
    "print('\\nmovement test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['WmTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1, train_firing_2], [train_EMG_0, train_EMG_1, train_EMG_2], test_firing, test_EMG, test_timestamps)\n",
    "print('\\niso test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['IsoTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1, train_firing_2], [train_EMG_0, train_EMG_1, train_EMG_2], test_firing, test_EMG, test_timestamps)\n",
    "print('\\nspring test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['SprTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1, train_firing_2], [train_EMG_0, train_EMG_1, train_EMG_2], test_firing, test_EMG, test_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, train_firing_0, train_EMG_0, _, _ = load_josh_mat(data['IsoTrain'])\n",
    "_, train_firing_1, train_EMG_1, _, _ = load_josh_mat(data['WmTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['IsoTest'])\n",
    "\n",
    "\n",
    "LSTM_rex([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the weighted version of the REx (doesn't change the function, just what's fed into the One Hot matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('iso and spring')\n",
    "\n",
    "\n",
    "_, train_firing_0, train_EMG_0, _, _ = load_josh_mat(data['IsoTrain'])\n",
    "_, train_firing_1, train_EMG_1, _, _ = load_josh_mat(data['SprTrain'])\n",
    "\n",
    "print('\\nmovement test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['WmTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps, weighted=True)\n",
    "print('\\niso test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['IsoTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps, weighted=True)\n",
    "print('\\nspring test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['SprTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps, weighted=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('iso and movement')\n",
    "\n",
    "_, train_firing_0, train_EMG_0, _, _ = load_josh_mat(data['IsoTrain'])\n",
    "_, train_firing_1, train_EMG_1, _, _ = load_josh_mat(data['WmTrain'])\n",
    "\n",
    "print('\\nmovement test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['WmTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps, weighted=True)\n",
    "print('\\niso test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['IsoTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps, weighted=True)\n",
    "print('\\nspring test')\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['SprTest'])\n",
    "LSTM_rex([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps, weighted=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VRex with weightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vrex_weighted(target, pred):\n",
    "    # weighting the risks based on the var of the EMG\n",
    "    # For this, we will bring in an array of Tx(2*M+1) \n",
    "    # where M is the number of muscles and the extra column is for a flag\n",
    "\n",
    "    # define the balance of mean and variance of risk\n",
    "    B = 0 #\n",
    "#     B = .5 #\n",
    "#     B = 1 #\n",
    "#     B = 5 #\n",
    "#     B = 10 #\n",
    "#     B = 50 #\n",
    "#     B = 100 #\n",
    "\n",
    "#     num_targets = K.shape(target)[1]//2 # number of cols / 2\n",
    "#     err = (target[:, 0:num_targets] - pred[:,0:num_targets]) # subtract the values\n",
    "#     se = K.square(err) * target[:,num_targets:-1] # multiply the square error by the gains\n",
    "#     mse = K.mean(se, axis=-1)\n",
    "\n",
    "    # number of muscles\n",
    "    muscles = target.shape[1]//2 # find that value of M\n",
    "    err = target[:,:muscles] - pred[:,:muscles] # find the error\n",
    "    se_musc = K.square(err) * target[:,num_targets:-1] # square error per muscle per timepoint\n",
    "    mse = tf.expand_dims(K.mean(se_musc,axis=-1),axis=-1) # take a weighted sum  of muscles per ts\n",
    "#     tf.print(f\"se shape:{se.shape}\")\n",
    "    \n",
    "    \n",
    "    # separate the conditions based on the flag\n",
    "    flag = target[:,-1] # last column\n",
    "#     risks = [K.mean(se[flag==C]) for C,_ in tf.unique(flag)]\n",
    "    u_flag,i_flag = tf.unique(flag) # for another one-hot matrix\n",
    "    flag_mask = tf.one_hot(i_flag, len(u_flag), axis=0) # mask for each condition -- CxT\n",
    "#     tf.print(f\"flag_mask shape: {flag_mask.shape}\")\n",
    "\n",
    "    split_se = tf.matmul(flag_mask, se) # this should give us a Cx1 array\n",
    "    # need to get the mean per condition, and samples/condition changes\n",
    "    # depending on the batch. the flag mask has already counted for us :)\n",
    "    risks = tf.math.divide(split_se, K.sum(flag_mask, axis=0))\n",
    "#     split_risk = tf.matmul(var_oh, mse_musc)\n",
    "#     weighted_risk = tf.multiply(tf.expand_dims(u_var, axis=-1), split_risk)\n",
    "#     risks = tf.math.divide(weighted_risk, K.sum(var_oh, axis=1)) # get the mean\n",
    "\n",
    "    rex = B*K.var(risks) + K.sum(risks)\n",
    "\n",
    "#     return rex\n",
    "    return mse\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Training on isometric and movement')\n",
    "\n",
    "_, train_firing_0, train_EMG_0, _, _ = load_josh_mat(data['IsoTrain'])\n",
    "_, train_firing_1, train_EMG_1, _, _ = load_josh_mat(data['WmTrain'])\n",
    "\n",
    "train_firing = {'Iso': train_firing_0, 'Move': train_firing_1}\n",
    "train_EMG = {'Iso': train_EMG_1, 'Move': train_EMG_1}\n",
    "\n",
    "# testing dictionaries\n",
    "test_firing = {}\n",
    "test_EMG = {}\n",
    "test_timestamps = {}\n",
    "# movement\n",
    "test_timestamps['Move'], test_firing['Move'], test_EMG['Move'], _, _ = load_josh_mat(data['WmTest'])\n",
    "test_timestamps['Iso'], test_firing['Iso'], test_EMG['Iso'], _, _ = load_josh_mat(data['IsoTest'])\n",
    "test_timestamps['Spr'], test_firing['Spr'], test_EMG['Spr'], _, _ = load_josh_mat(data['SprTest'])\n",
    "\n",
    "# Send it\n",
    "LSTM_rex_weighted(train_firing, train_EMG, test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "# print('\\nmovement test')\n",
    "# test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['WmTest'])\n",
    "# LSTM_rex_weighted([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)\n",
    "# print('\\niso test')\n",
    "# test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['IsoTest'])\n",
    "# LSTM_rex_weighted([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)\n",
    "# print('\\nspring test')\n",
    "# test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['SprTest'])\n",
    "# LSTM_rex_weighted([train_firing_0, train_firing_1], [train_EMG_0, train_EMG_1], test_firing, test_EMG, test_timestamps)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch Space\n",
    "Sometimes the variable viewer isn't very goood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting bits of code to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/LSTM_exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_records([log])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.DataFrame.from_records([log]),pd.DataFrame.from_records([log])], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_MC = linear_model.LinearRegression()\n",
    "\n",
    "VAF_cols = [cols for cols in log.columns if \"VAF\" in cols]\n",
    "reg_cols = [cols for cols in log.columns if \"VAF\" not in cols]\n",
    "\n",
    "mdl_MC.fit(log[reg_cols],log[VAF_cols])\n",
    "\n",
    "# for iter,col in enumerate(reg_cols):\n",
    "#     print(col)\n",
    "#     print(f\"\\t{mdl_MC.coef_[:,iter]/np.mean(log[col])}\\n\")\n",
    "\n",
    "pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "range = np.linspace(np.min(log['drop_in']), np.max(log['drop_in']),2)\n",
    "\n",
    "for iter,col in enumerate(VAF_cols):\n",
    "    vals = mdl_MC.coef_[iter,1]*range + mdl_MC.intercept_[iter]\n",
    "    ax.plot(range, vals, label=col)\n",
    "    ax.legend()\n",
    "    ax.scatter(log['drop_in'], log[col])\n",
    "    ax.set_xlabel('Input Dropout Percentage')\n",
    "    ax.set_ylabel('Coefficient of Determination')\n",
    "    ax.set_ylim([-.1, 1.1])\n",
    "    for spine in ['top','bottom','left','right']:\n",
    "        ax.spines[spine].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "for col in VAF_cols:\n",
    "    ax.scatter(log['n_units'], log[col], label=col)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./monte_carlo_log_30runs_4fxval.pkl','wb') as fid:\n",
    "          pickle.dump(log,fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.array([[1,2,3],[1,2,3],[1,2,3]], dtype=float)\n",
    "\n",
    "np.mean(aa, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_timestamps, train_firing, train_EMG, train_force, train_kin = load_josh_mat(data['WmTrain'])\n",
    "test_timestamps, test_firing, test_EMG, test_force, test_kin = load_josh_mat(data['WmTest'])\n",
    "\n",
    "\n",
    "layer_0_units = 200\n",
    "drop_in = .25     # input dropout percentage for LSTM layer\n",
    "drop_rec = .25    # recurrent dropout for LSTM\n",
    "drop_lay = .25    # dropout layer?\n",
    "\n",
    "# # input hyper params -- reshape input vector\n",
    "# batch_size = 64 # why not? will test to see training accuracy after\n",
    "# seq_len = train_firing.shape[0]//batch_size # splitting out the batches\n",
    "n_neurons = train_firing.shape[1] # number of neurons\n",
    "n_EMGs = train_EMG.shape[1]\n",
    "seq_len = 10\n",
    "rnn_train_i = np.zeros((train_firing.shape[0],seq_len, train_firing.shape[1]))\n",
    "rnn_test_i = np.zeros((test_firing.shape[0],seq_len, test_firing.shape[1]))\n",
    "rnn_train_o = train_EMG.to_numpy()\n",
    "rnn_test_o = test_EMG.to_numpy()\n",
    "\n",
    "# normalize the EMGs\n",
    "EMG_std = np.std(rnn_train_o, axis=0)\n",
    "for ii in np.arange(rnn_train_o.shape[1]):\n",
    "    rnn_train_o[:,ii] = rnn_train_o[:,ii]/EMG_std[ii]\n",
    "    rnn_test_o[:,ii] = rnn_test_o[:,ii]/EMG_std[ii]\n",
    "\n",
    "# create the sequences\n",
    "for ii in np.arange(10):\n",
    "    rnn_train_i[:,ii,:] = train_firing.shift(-ii, fill_value=0).to_numpy()\n",
    "    rnn_test_i[:,ii,:] = test_firing.shift(-ii, fill_value=0).to_numpy()\n",
    "\n",
    "# add the \"condition\" term\n",
    "conds_rng = np.random.default_rng()\n",
    "condition_train = conds_rng.choice(np.arange(4),(rnn_train_o.shape[0],1))\n",
    "# condition_train = np.ones((rnn_train_i.shape[0],1))\n",
    "rnn_train_o = np.append(rnn_train_o, condition_train, axis=-1)\n",
    "condition_test = np.ones((rnn_test_o.shape[0],1))\n",
    "rnn_test_o = np.append(rnn_test_o, condition_test, axis=-1)\n",
    "\n",
    "# Set up the LSTMs\n",
    "mdl = tf.keras.models.Sequential()\n",
    "\n",
    "\n",
    "\n",
    "mdl.add(tf.keras.layers.LSTM(layer_0_units, input_shape = (seq_len,n_neurons), dropout=drop_in, recurrent_dropout=drop_rec))\n",
    "if drop_lay:\n",
    "    mdl.add(tf.keras.layers.Dropout(drop_lay)) # dropout layer if wanted\n",
    "mdl.add(tf.keras.layers.Dense(rnn_train_o.shape[1], activation='relu')) # dense combination layer\n",
    "# mdl.add(tf.keras.layers.Dense(rnn_train_o.shape[1])) # dense combination layer\n",
    "mdl.compile(loss=vrex_loss, optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "mdl.fit(rnn_train_i, rnn_train_o, epochs=40, verbose=True, batch_size=128)\n",
    "\n",
    "\n",
    "train_pred = mdl.predict(rnn_train_i)[:,:-1]\n",
    "test_pred = mdl.predict(rnn_test_i)[:,:-1]\n",
    "train_VAFs = metrics.explained_variance_score(rnn_train_o[:,:-1], train_pred, multioutput='raw_values')\n",
    "test_VAFs = metrics.explained_variance_score(rnn_test_o[:,:-1], test_pred, multioutput='raw_values')\n",
    "\n",
    "print('----------------------------------------')\n",
    "for ii in np.arange(len(train_EMG.columns)):\n",
    "    print(train_EMG.columns[ii])\n",
    "    print(f\"\\tTrain VAF: {train_VAFs[ii]}\")\n",
    "    print(f\"\\tTest VAF: {test_VAFs[ii]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vREx loss function\n",
    "def vrex_loss(target, pred):\n",
    "    # from the Risk Extrapolation paper\n",
    "    B = 3 #\n",
    "    \n",
    "    err = target[:,:-1] - pred[:,:-1] # without the condition flag\n",
    "    se = K.square(err) # squared error\n",
    "    mse = K.mean(se, axis=-1) # mean squared error for each sample\n",
    "    \n",
    "    # now create a one-hot matrix for timestamps with a one for the appropriate condition\n",
    "    conditions = target[:,-1] # condition flag\n",
    "    u_conds,i_conds = tf.unique(conditions)\n",
    "    cond_oh = tf.transpose(tf.one_hot(i_conds, len(u_conds)))\n",
    "    \n",
    "    # risk for each condition -- ie MSE for each condition\n",
    "    risk = tf.matmul(cond_oh,se)\n",
    "    \n",
    "    rex = B*K.var(risk) + K.sum(risk)\n",
    "\n",
    "#     num_targets = K.shape(target)[1]//2 # number of cols / 2\n",
    "#     err = (target[:, 0:num_targets] - pred[:,0:num_targets]) # subtract the values\n",
    "#     se = K.square(err) * target[:,num_targets:] # multiply the square error by the gains\n",
    "#     mse = K.mean(se, axis=-1)\n",
    "    \n",
    "#     tf.print(mse.shape)\n",
    "\n",
    "\n",
    "    return rex\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = {'123':123,'456':456,'789':789}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii,a in enumerate(aa.items()):\n",
    "    print(f\"{ii}:{a[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(aa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6f5ab44297089514ffa059ea0d83d6392bebfea7e191fb6b7f26c8412c9553b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
